{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 12.1: Inferring the probability of a coin toss from data\n",
    "\n",
    "This is our first Bayesian inference example!\n",
    "\n",
    "Let's say that we run a coin toss experiment $N$ times and we wish to figure out the probability of heads.\n",
    "We will bring into the picture all the mathematical machinery we have introduced so far.\n",
    "Let's say that the data we have observe are $x_1,\\dots,x_N$.\n",
    "For notational convenience we will be writing:\n",
    "$$\n",
    "x_{1:N} := (x_1,\\dots,x_N).\n",
    "$$\n",
    "\n",
    "First, let's start with the probability of success of the coin toss.\n",
    "Let's call it $\\theta$.\n",
    "How can we describe our uncertainty about it?\n",
    "We have to assign a *prior* probability distribution on it.\n",
    "Let's say that we don't know anything about it except that it must be between 0 and 1.\n",
    "What distribution should we assign?\n",
    "Of course, a uniform distribution:\n",
    "$$\n",
    "\\theta \\sim U([0,1]).\n",
    "$$\n",
    "Second, each coin toss experiment corresponds to an independent Bernoulli variable with the same probability of success $\\theta$.\n",
    "We write:\n",
    "$$\n",
    "X_n | \\theta \\sim \\operatorname{Bernoulli}(\\theta),\n",
    "$$\n",
    "for $n=1,\\dots,N$.\n",
    "Note that these random variables depend on $\\theta$.\n",
    "That's why we are conditioning like this.\n",
    "\n",
    "Before proceeding with the mathematics, let's draw the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First for one coin toss\n",
    "from graphviz import Digraph\n",
    "gc = Digraph('coin_toss_bayes_1')\n",
    "gc.node('theta', label='<&theta;>')\n",
    "gc.node('X1', label='<X<sub>1</sub>>', style='filled')\n",
    "gc.edge('theta', 'X1')\n",
    "gc.render('coin_toss_bayes_1', format='png')\n",
    "gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for two coin tosses\n",
    "gc2 = Digraph('coin_toss_bayes_2')\n",
    "gc2.node('theta', label='<&theta;>')\n",
    "gc2.node('X1', label='<X<sub>1</sub>>', style='filled')\n",
    "gc2.node('X2', label='<X<sub>2</sub>>', style='filled')\n",
    "gc2.edge('theta', 'X1')\n",
    "gc2.edge('theta', 'X2')\n",
    "gc2.render('coin_toss_bayes_2', format='png')\n",
    "gc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for three coin tosses\n",
    "gc3 = Digraph('coin_toss_bayes_3')\n",
    "gc3.node('theta', label='<&theta;>')\n",
    "gc3.node('X1', label='<X<sub>1</sub>>', style='filled')\n",
    "gc3.node('X2', label='<X<sub>2</sub>>', style='filled')\n",
    "gc3.node('X3', label='<X<sub>3</sub>>', style='filled')\n",
    "gc3.edge('theta', 'X1')\n",
    "gc3.edge('theta', 'X2')\n",
    "gc3.edge('theta', 'X3')\n",
    "gc3.render('coin_toss_bayes_3', format='png')\n",
    "gc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this notation gets a little bit too involved, we introduce the so called [plate notation](https://en.wikipedia.org/wiki/Plate_notation).\n",
    "Whategver is inside the subgrpah indicated by the box is supposed to be repeated as many times as indicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp = Digraph('coin_toss_bayes_plate')\n",
    "gcp.node('theta', label='<&theta;>')\n",
    "with gcp.subgraph(name='cluster_0') as sg:\n",
    "    sg.node('Xn', label='<X<sub>n</sub>>', style='filled')\n",
    "    sg.attr(label='n=1,...,N')\n",
    "    sg.attr(labelloc='b')\n",
    "gcp.edge('theta', 'Xn')\n",
    "gcp.render('coin_toss_bayes_plate', format='png')\n",
    "gcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To carry out Bayesian inference, we need the joint probability density of all variables.\n",
    "It is:\n",
    "$$\n",
    "p(x_1,\\dots,x_N, \\theta) = p(x_{1:N}|\\theta) p(\\theta) = \\left(\\prod_{n=1}p(x_n|\\theta)\\right)p(\\theta),\n",
    "$$\n",
    "where we first used Bayes' rule and then the fact that the coin tosses are independent.\n",
    "We must find the mathematical form of all these expressions.\n",
    "For $\\theta$, we simply have:\n",
    "$$\n",
    "p(\\theta) = 1_{[0,1]}(\\theta),\n",
    "$$\n",
    "where $1_A(x)$ is the indicator function of $A$, i.e., $1_A(x) = 1$ if $x$ is in $A$ and zero otherwise.\n",
    "For the Bernoulli pmf's we have:\n",
    "$$\n",
    "p(X_n = 1|\\theta) = \\theta,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p(X_n = 0|\\theta) = 1- \\theta.\n",
    "$$\n",
    "So, in a unified way, we can write:\n",
    "$$\n",
    "p(x_n | \\theta) = \\theta^{x_n}(1-\\theta)^{1-x_n}.\n",
    "$$\n",
    "Now, let's re-write the joint pmf:\n",
    "$$\n",
    "p(x_1,\\dots,x_N, \\theta) = \\theta^{\\sum_{n=1}^Nx_n}(1-\\theta)^{N-\\sum_{n=1}^Nx_n}1_{[0,1]}(\\theta),\n",
    "$$\n",
    "which has a nice interpretation as it depends only on the total number of heads $\\sum_{n=1}^Nx_n$.\n",
    "\n",
    "Now, we are in a position to apply Bayes rule to condition on the data.\n",
    "We have:\n",
    "$$\n",
    "p(\\theta|x_{1:N}) = \\frac{p(x_{1:N},\\theta)}{p(x_{1:N})} \\propto p(x_{1:N}, \\theta) = \\theta^{\\sum_{n=1}^Nx_n}(1-\\theta)^{N-\\sum_{n=1}^Nx_n}1_{[0,1]}(\\theta).\n",
    "$$\n",
    "It may be the first time you encounter this, but we have actually discovered a new distribution called the [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution).\n",
    "This is what the posterior turns out to be.\n",
    "This is one of the few instances where the posterior is analytically available.\n",
    "\n",
    "Now, that we know about the Beta distribution, we can write for the posterior of $\\theta$:\n",
    "$$\n",
    "p(\\theta|x_{1:N}) = \\operatorname{Beta}\\left(\\theta\\middle|1 + \\sum_{n=1}^Nx_n, 1 + N - \\sum_{n=1}^Nx_n\\right).\n",
    "$$\n",
    "where with $\\operatorname{Beta}(\\theta|\\alpha,\\beta)$ we mean the PDF of the $\\operatorname{Beta}(\\alpha,\\beta)$ evaluated at $\\theta$ (this is a very useful notation).\n",
    "So, we see that the $\\alpha$ parameter is just one plus the number of heads and the $\\beta$ parameter is one plus the number of tails.\n",
    "\n",
    "Let's try this out with some fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "# Take a fake coin which is a little bit biased\n",
    "theta_true = 0.8\n",
    "# This is the random variable corresponding to a coin toss\n",
    "X = st.bernoulli(theta_true)\n",
    "# Sample from it a number of times to generate our data = (x1, ..., xN)\n",
    "N = 200\n",
    "data = X.rvs(size=N)\n",
    "# Now we are ready to calculate the posterior which the Beta we have above\n",
    "alpha = 1.0 + data.sum()\n",
    "beta = 1.0 + N - data.sum()\n",
    "Theta_post = st.beta(alpha, beta)\n",
    "# Now we can plot the posterior PDF for theta\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "thetas = np.linspace(0, 1, 100)\n",
    "ax.plot([theta_true], [0.0], 'o', markeredgewidth=2, markersize=10, label='True value')\n",
    "ax.plot(thetas, Theta_post.pdf(thetas), label=r'$p(\\theta|x_{1:N})$')\n",
    "ax.set_xlabel(r'$\\theta$')\n",
    "ax.set_title('$N={0:d}$'.format(N))\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Experiment with different values of $\\theta_{\\text{true}}$ and different values of $N$.\n",
    "+ Is the true value always covered by the posterior PDF?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
