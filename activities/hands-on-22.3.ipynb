{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 22.3: Tuning the Hyperparameters\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Tune the hyper-parameters of a Gaussian process using maximum marginal likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "+ [Chapter 5 of GP for ML textbook](http://www.gaussianprocess.org/gpml/chapters/RW5.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Gaussian process regression with fitted hyperparameters\n",
    "\n",
    "Let's generate synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-68e59c99b468>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# The true function that we will try to identify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Some data to train on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkeredgewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f_true' is not defined"
     ]
    }
   ],
   "source": [
    "# Fixing the seed so that we all see the same data\n",
    "np.random.seed(1234)\n",
    "n = 10\n",
    "# The inputs are in [0, 1]\n",
    "X = np.random.rand(n, 1) # Needs to be an n x 1 vector\n",
    "# The outputs are given from a function plus some noise\n",
    "# The standard deviation of the noise is:\n",
    "sigma = 0.4\n",
    "# The true function that we will try to identify\n",
    "# Some data to train on\n",
    "Y = f_true(X) + sigma * np.random.randn(X.shape[0], 1)\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a squared exponential kernel and make a model with Gaussian likelihood (the default choice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "k = GPy.kern.RBF(1)\n",
    "gpm = GPy.models.GPRegression(X, Y, k)\n",
    "print(gpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain what all this means.\n",
    "Notice that there are some default values for the hyperparameters (they are all one).\n",
    "Also, notice that ``GPy`` is keeping track of how many parameters it needs to fit.\n",
    "Here we have three parameters ($s,\\ell$ and $\\sigma$).\n",
    "The second column are constraints for the parameters.\n",
    "The ``+ve`` term means that the corresponding hyperparamer has to be positive.\n",
    "Notice that there is nothing in the ``priors`` column.\n",
    "This is because we have set no priors right now.\n",
    "When this happens, ``GPy`` assumes that we assign a flat prior, i.e., here it assumes that we have assigned $p(\\theta)\\propto 1$ and $p(\\sigma)\\propto 1$.\n",
    "That's not the best choice, but it should be ok for now.\n",
    "\n",
    "Now, pay attention to the ``Objective``. This is the $-\\log p(\\theta,\\sigma|\\mathcal{D})$ for the current choice of parameters.\n",
    "Let's now find the MAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpm.optimize(messages=True)\n",
    "print(gpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. We did find something with higher posterior value.\n",
    "Let's plot the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "m_star, v_star = gpm.predict(x_star)\n",
    "f_lower = m_star - 2.0 * np.sqrt(v_star - gpm.likelihood.variance)\n",
    "f_upper = m_star + 2.0 * np.sqrt(v_star - gpm.likelihood.variance)\n",
    "y_lower = m_star - 2.0 * np.sqrt(v_star)\n",
    "y_upper = m_star + 2.0 * np.sqrt(v_star)\n",
    "ax.fill_between(x_star.flatten(), y_lower.flatten(), y_upper.flatten(), color='red', alpha=0.25, label='$y^*$ 95% pred.')\n",
    "ax.fill_between(x_star.flatten(), f_lower.flatten(), f_upper.flatten(), alpha=0.5, label='$f(\\mathbf{x}^*)$ 95% pred.')\n",
    "ax.plot(x_star, m_star, lw=2, label='$m_n(x)$')\n",
    "ax.plot(x_star, f_true(x_star), 'm-.', label='True function')\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admitidly, this doesn't look very good.\n",
    "(Of course, we can tell this only because we know the truth).\n",
    "It seems that the assigned lengthscale is too small.\n",
    "Also, the likelihood variance seems smaller than it really is.\n",
    "What do we do now?\n",
    "You have two choices:\n",
    "+ You encode some prior knowledge and repeat.\n",
    "+ You add some more data and repeat.\n",
    "\n",
    "Let's start with some prior knowledge and let the other item for the questions section.\n",
    "Let's say that we know that the noise variance.\n",
    "How do we encode this?\n",
    "Here you go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpm.likelihood.variance.constrain_fixed(sigma ** 2)\n",
    "print(gpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it now ``GPy`` reports that the likelihood variance is fixed.\n",
    "Let's repeat the optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpm.optimize(messages=True)\n",
    "print(gpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "m_star, v_star = gpm.predict(x_star)\n",
    "f_lower = m_star - 2.0 * np.sqrt(v_star - gpm.likelihood.variance)\n",
    "f_upper = m_star + 2.0 * np.sqrt(v_star - gpm.likelihood.variance)\n",
    "y_lower = m_star - 2.0 * np.sqrt(v_star)\n",
    "y_upper = m_star + 2.0 * np.sqrt(v_star)\n",
    "ax.fill_between(x_star.flatten(), y_lower.flatten(), y_upper.flatten(), color='red', alpha=0.25, label='$y^*$ 95% pred.')\n",
    "ax.fill_between(x_star.flatten(), f_lower.flatten(), f_upper.flatten(), alpha=0.5, label='$f(\\mathbf{x}^*)$ 95% pred.')\n",
    "ax.plot(x_star, m_star, lw=2, label='$m_n(x)$')\n",
    "ax.plot(x_star, f_true(x_star), 'm-.', label='True function')\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better.\n",
    "But it seems that the automatically selected lengthscale is smaller than the true one.\n",
    "(Of course, in reality, we don't really know what the true lengthscale is).\n",
    "Let's assign a prior probability density on the lengthscale which pushes it to be greater.\n",
    "Since we are dealing with a positie parameter and we don't know much about it, let's assign an exponential prior with a rate of 2 (which will yield an expectation of 0.5):\n",
    "$$\n",
    "\\ell \\sim \\operatorname{Log-N}(0.2, 1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell_prior = GPy.priors.LogGaussian(.2, 1.0)\n",
    "# Let's visualize it to make sure it's ok\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ells = np.linspace(0.01, 2.0, 100)\n",
    "ax.plot(ells, ell_prior.pdf(ells))\n",
    "ax.set_xlabel('$\\ell$')\n",
    "ax.set_ylabel('$p(\\ell)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here is how you can set it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpm.kern.lengthscale.set_prior(ell_prior)\n",
    "print(gpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpm.optimize(messages=True)\n",
    "print(gpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "m_star, v_star = gpm.predict(x_star)\n",
    "f_lower = m_star - 2.0 * np.sqrt(v_star - gpm.likelihood.variance)\n",
    "f_upper = m_star + 2.0 * np.sqrt(v_star - gpm.likelihood.variance)\n",
    "y_lower = m_star - 2.0 * np.sqrt(v_star)\n",
    "y_upper = m_star + 2.0 * np.sqrt(v_star)\n",
    "ax.fill_between(x_star.flatten(), y_lower.flatten(), y_upper.flatten(), color='red', alpha=0.25, label='$y^*$ 95% pred.')\n",
    "ax.fill_between(x_star.flatten(), f_lower.flatten(), f_upper.flatten(), alpha=0.5, label='$f(\\mathbf{x}^*)$ 95% pred.')\n",
    "ax.plot(x_star, m_star, lw=2, label='$m_n(x)$')\n",
    "ax.plot(x_star, f_true(x_star), 'm-.', label='True function')\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better, but not perfect.\n",
    "But remember: You don't know what the truth is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Let's investigate what happens to the previous examples as you increase the number of observations.\n",
    "\n",
    "+ Rerun everything gradually increasing the number of samples from $n=10$ to $n=100$.\n",
    "Notice that as the number of samples increases it doesn't really matter what your prior knowledge is.\n",
    "As a matter of fact, for the largest number of samples you try, pick a very wrong prior probability for $\\ell$.\n",
    "See what happens.\n",
    "\n",
    "+ Rerun everything with $\\sigma=0.01$ (small noise) and gradually increasing the number of samples from $n=10$ to $n=100$.\n",
    "For small noise, the model is trying to interpolate.\n",
    "Is it capable of figuring out that the noise is small when the number of observations is limited? When does the method realize the noise is indeed small?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
