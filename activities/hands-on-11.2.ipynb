{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 11.2: The principle of maximum entropy for discrete random variables\n",
    "\n",
    "## Objectives\n",
    "+ Learn how to find the maximum entropy distribution for discrete random variables.\n",
    "\n",
    "## The Brandeis Dice Problem\n",
    "This is from the 1962 Brandeis lectures of E. T. Jaynes.\n",
    "\n",
    "> When a die is tossed, the number of spots up can have any value $x$ in $1,\\dots,6$. Suppose a die has been tossed $N$ times and we are told only that the average number of spots up was not $3.5$ (as we might expect from an \"honest\" die) but 4.5. Given this information, <u>and nothing else</u>, what probability should we assign to $x$ spots on the next toss?\n",
    "\n",
    "Let $X$ be a random variable corresponding to the result of tossing the die.\n",
    "The description above imposes the following mean value constraint on the random variable $X$:\n",
    "$$\n",
    "\\sum_{x=1}^6 x p(x) = 4.5.\n",
    "$$\n",
    "As we discussed in the lecture, to come up with a probability mass function for $X$ you have to maximize the entropy subject to the constraints above.\n",
    "We saw that this constrained optimization problem has a unique solution of the form:\n",
    "$$\n",
    "p(x) = \\frac{\\exp\\{\\lambda x\\}}{Z(\\lambda)},\n",
    "$$\n",
    "where $Z(\\lambda)$ is the *partition function*:\n",
    "$$\n",
    "Z(\\lambda) = \\sum_{i}e^{\\lambda i} = e^{\\lambda} + e^{2\\lambda} + \\dots + e^{6\\lambda},\n",
    "$$\n",
    "and $\\lambda$ is a parameter to be tuned so that the constraint is satisfied.\n",
    "We will identify $\\lambda$ by solving a root finding problem.\n",
    "To this end, let us write the partition function as:\n",
    "$$\n",
    "Z(\\lambda) = \\left(e^{\\lambda}\\right)^1+\\left(e^{\\lambda}\\right)^2 + \\dots + \\left(e^{\\lambda}\\right)^6.\n",
    "$$\n",
    "According to the theory, in order to find $\\lambda$ we must solve:\n",
    "$$\n",
    "\\frac{\\partial \\log Z}{\\partial \\lambda} = 4.5.\n",
    "$$\n",
    "We are going to use sympy to find the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sm\n",
    "s_lam = sm.Symbol('lambda')\n",
    "s_Z = 0\n",
    "for n in range(1, 7):\n",
    "    s_Z += sm.exp(n * s_lam)\n",
    "s_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and here is the derivative of the logarithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_dZdlam = sm.diff(sm.log(s_Z), s_lam)\n",
    "s_dZdlam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is nothing more but the expectation of $x$.\n",
    "That's super easy to program:\n",
    "Let's solve this root-finding problem numerically using the [Brent's method](https://en.wikipedia.org/wiki/Brent%27s_method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "# The function of which the root we want to find\n",
    "def f(lam):\n",
    "    p_unormalized = np.exp(np.arange(1, 7) * lam)\n",
    "    p = p_unormalized / np.sum(p_unormalized)\n",
    "    E_X = np.sum(np.arange(1, 7) * p)\n",
    "    return E_X - 4.5\n",
    "\n",
    "# Left bound for x\n",
    "a = -2\n",
    "# Right bound for x\n",
    "b = 2\n",
    "res = scipy.optimize.root_scalar(f, bracket=(a,b), method='brentq',\n",
    "                                 xtol=1e-20,\n",
    "                                 rtol=1e-15)\n",
    "print(res)\n",
    "lam = res.root\n",
    "print('Lambda = {0:1.2f}'.format(lam))\n",
    "# The maximum entropy probabilities\n",
    "p = np.exp(lam * np.arange(1, 7))\n",
    "p = p / np.sum(p)\n",
    "print('p = ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the expectation turns out to be correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(p * np.arange(1, 7)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "plt.bar(np.arange(1, 7), p, alpha=0.5)\n",
    "ax.set_xlabel('Die result ($x$)')\n",
    "ax.set_ylabel('Probability $p(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "+ Rerun the code above assuming that the mean is 3.5. What kind of distribution do you find? Why?\n",
    "+ If you have some time to spare, modify the example above to add the constraint that the variance of $X$ should be 0.2. Hint: First, translate the constraint about the variance to a constraint about $\\mathbb{E}[X^2]$. Second, you need to introduce one more parameter to optimize for. Call it $\\mu$. The distribution would be $p(x) = \\frac{\\exp\\{\\lambda x + \\mu x^2\\}}{Z(\\lambda,\\mu)}$. Then derive the set of non-linear equations you need solve to find $\\lambda$ and $\\mu$ by expanding these two equations:\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial \\lambda} = \\mathbb{E}[X],\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial \\mu} = \\mathbb{E}[X^2].\n",
    "$$\n",
    "Finally, use [scipy.optimize.root](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.root.html#scipy.optimize.root) to solve the root-finding problem.\n",
    "Be careful with this because it could take several hours to do right..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
