{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Activity 27 - Sampling Methods\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Define the basics of Markov Chains (transition kernels, invariance, ergodicity, detailed balance condition).\n",
    "+ Define Mark\n",
    "\n",
    "## References\n",
    "\n",
    "+ These notes.\n",
    "+ Chapter 11 of Bishop 2006.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "We have seen that the Bayesian formulation of inverse problems results in intractable posterior distributions.\n",
    "In particular, these posteriors are known only up to a normalization constant.\n",
    "In the next series of lectures, we are going to develop methodologies that allows us to sample from these distributions.\n",
    "The most celebrated of these methodologies is Markov Chain Monte Carlo (MCMC) which has at its core the Metropolis algorithms.\n",
    "It is a long way to go, but we can state right away the problem definition and what we are set out to do.\n",
    "\n",
    "Without loss of generality, let $X\\in\\mathcal{X}\\subset\\mathbb{R}^d$ be random variable with an arbitrary probability density, say $\\pi(x)$ known up to a normalization constant.\n",
    "That is, we have that:\n",
    "$$\n",
    "\\pi(x) = \\frac{h(x)}{Z},\n",
    "$$\n",
    "where $h(x)$ is a known function that we can evaluate at will, but $Z$ is not known.\n",
    "Our goal is to generate samples from $\\pi(x)$, by only evaluating $h(x)$.\n",
    "The revolutionary idea of Metropolis was to construct a stochastic process using only $h(x)$ samples from which resemble (in some way we will specify below) samples from $\\pi(x)$.\n",
    "To understand the details, we will have to introduce some key concepts.\n",
    "\n",
    "## Markov Chains\n",
    "\n",
    "### Definition\n",
    "Let $X_n, n=1,2,\\dots$ be a stochastic process taking values in $\\mathcal{X}\\subset\\mathbb{R}^d$ whcih could either be discrete or continuous.\n",
    "We will refer to $n$ as the *time step*.\n",
    "We say that this stochastic process is a *Markov chain* if the evolution of $X_{n+1}$ depends only on the value of $X_n$ and not on all the history of the process.\n",
    "Let us define this a little bit more rigorously.\n",
    "\n",
    "Let $x_1,\\dots,x_n\\in\\mathcal{X}$ be the observed values of the process up to $n$-th time step.\n",
    "The Markov property can now be expressed as:\n",
    "$$\n",
    "p(X_{n+1}=x_{n+1}|X_1=x_1,\\dots,X_n=x_n) = p(X_{n+1}=x_{n+1}|X_n=x_n).\n",
    "$$\n",
    "If there is no ambiguity, we will be simplifying the notation by dropping the capital letters.\n",
    "That is, we will be writting:\n",
    "$$\n",
    "p(x_{n+1}|x_1,\\dots,x_n) = p(x_{n+1}|x_n).\n",
    "$$\n",
    "To simplify things even further, we will also use the collective notation:\n",
    "$$\n",
    "x_{1:n} = (x_1,\\dots,x_n)\\in\\mathcal{X}^n.\n",
    "$$\n",
    "With this notation, we can re-write the Markov property in even simpler terms:\n",
    "$$\n",
    "p(x_{n+1}|x_{1:n}) = p(x_{n+1}|x_n).\n",
    "$$\n",
    "\n",
    "### The joint distribution of a Markov chain\n",
    "The *joint distribution* is defined as:\n",
    "$$\n",
    "p(x_{1:n}) := P(X_1=x_1,\\dots,X_n=x_n).\n",
    "$$\n",
    "If $X_n$ is a Markov chain, then we simply have:\n",
    "$$\n",
    "p(x_{1:n}) = p(x_1)p(x_2|x_1)\\dots p(x_n|x_{n-1}),\n",
    "$$\n",
    "or\n",
    "$$\n",
    "p(x_{1:n}) = p(x_1)\\prod_{t=2}^np(x_t|x_{t-1}).\n",
    "$$\n",
    "So, we see that we know the joint distribution of a Markov chain if we know the probability of hoping from one state to the next.\n",
    "This propability is known as the transition kernel of the Markov chain.\n",
    "\n",
    "### Transition Kernel\n",
    "To describe a Markov chain, we only need to know the *transition kernel*.\n",
    "The transition kernel gives the probability of moving from one state to any other at a given step.\n",
    "Mathematically, the transition kernel of the $n$-th step is the function:\n",
    "$$\n",
    "T_n:\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}_+,\n",
    "$$\n",
    "defined by:\n",
    "$$\n",
    "T_n(x_n, x_{n+1}) = P(X_{n+1}=x_{n+1}|X_n=x_n).\n",
    "$$\n",
    "In words, $T_n(x_n, x_{n+1})$ is the probability that at step $n$ we jump from state $x_n$ to to state $x_{n+1}$.\n",
    "\n",
    "Please, note that the transition kernel, in general, depends on the time step $n$.\n",
    "We say that the Markov chain is *stationary*, if the transition kernel does not depend on $n$, i.e., if\n",
    "$$\n",
    "T_n(x_n, x_{n+1}) = T(x_n,x_{n+1}).\n",
    "$$\n",
    "**From now on, we will only consider stationary Markov chains.**\n",
    "For stationary Markov chains, and when there is now ambiguity, we will be writing:\n",
    "$$\n",
    "T(x_n,x_{n+1}) = p(x_{n+1}|x_n).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariant Distributions\n",
    "\n",
    "Let $X_1,X_2,\\dots$ be a Markov chain with transition kernel $T(x,x')$ and $\\pi(x)$ be a probability density.\n",
    "We say that the Markov chain leaves $\\pi(x)$ *invariant* if:\n",
    "$$\n",
    "\\pi(x) = \\int \\pi(x')T(x',x)dx'.\n",
    "$$\n",
    "In words, $\\pi(x)$ is invariant if when you start from a sample from it and you follow the Markov chain you get a sample from it.\n",
    "\n",
    "Invariance is one of the key requirements of a working MCMC algorithm.\n",
    "Whatever you do, the chain you construct must be invariant with respect to the distribution from which you want to sample.\n",
    "If you manage to do that, then once you get one sample from your distribution, you can get as many as you want by simply following the transition kernel.\n",
    "\n",
    "\n",
    "\n",
    "## The Detailed Balance Condition\n",
    "Checking invariance is not trivial for a generic Markov chain.\n",
    "However, there is a sufficient condition that guarantees invariance.\n",
    "This condition is known as the *detailed balance condition* and it is:\n",
    "$$\n",
    "\\pi(x)T(x,x') = \\pi(x')T(x',x).\n",
    "$$\n",
    "In words, a Markov chain that satisfies the detailed balance condition is *reversible* in the following sense.\n",
    "The probability of sampling an $x$ and transitioning to $x'$ is the same as the probability of doing the reverse.\n",
    "\n",
    "If the detailed balance condition is satisfied, then $\\pi(x)$ is an invariant distribution:\n",
    "$$\n",
    "\\int \\pi(x')T(x',x)dx' = \\int \\pi(x)T(x,x')dx' = \\pi(x)\\int T(x,x')dx' = \\pi(x),\n",
    "$$\n",
    "since\n",
    "$$\n",
    "\\int T(x,x') dx' = \\int p(x'|x)dx' = 1.\n",
    "$$\n",
    "The reverse does not necessarily hold.\n",
    "The key idea of Metropolis was to construct a Markov chain that satisfies the detailed balance condition for the distribution you are interested in.\n",
    "\n",
    "## Ergodicity\n",
    "\n",
    "A Markov chain may have no invariant distribution (e.g., the random walk does not have an invariant distribution), one, or more than one.\n",
    "To guarantee uniqueness of the invariant distribution, need *ergodicity*.\n",
    "To define ergodicity precicely for contiuous Markov chains, we need a little bit of notation.\n",
    "In words, ergodicity requires that the random variable $X_n$ converges in distribution to $\\pi(x)$ irrespective of the starting point.\n",
    "Obviously, this is not easy to show for a generic Markov chain.\n",
    "Fortunately, we know that a Markov chain is ergodic if:\n",
    "\n",
    "+ it is *aperiodic* (i.e., it does not return to the same state at fixed intervals)\n",
    "+ it is *positive recurrent* (i.e., the expected number of steps for returning to the same state is finite).\n",
    "\n",
    "## Equilibrium Distribution\n",
    "If a Markov chain is ergodic and it has an invariant distribution, then that invariant distribution is unique and it is called the *equilibrium distribution*.\n",
    "The Metropolis algorithms constructs a Markov chain that has a desired equilibrium distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Metropolis Algorithm\n",
    "\n",
    "Now, let's get back to the initial problem of sampling from:\n",
    "$$\n",
    "\\pi(x) = \\frac{h(x)}{Z},\n",
    "$$\n",
    "without knowing $Z$.\n",
    "In 1953, Metropolis et al. demonstrated how we can construct a Markov chain with $\\pi(x)$ as the equilibrium density.\n",
    "The algorithm is based on biasing an underlying symmetric, stationary Markov chain.\n",
    "Let $T(x,x')$ be the transition kernel of this underlying Markov chain (also called the *proposal distribution*.\n",
    "The transition kernel must be symmetric, i.e.,\n",
    "$$\n",
    "T(x,x') = T(x',x).\n",
    "$$\n",
    "A very common choice of the proposal distribution is the random walk transition kernel:\n",
    "$$\n",
    "T(x,x') = \\mathcal{N}(x'|x, \\Sigma).\n",
    "$$\n",
    "However, this is just one possibility.\n",
    "Once we have pick a proposal, we construct the desired Markov chain as follows:\n",
    "\n",
    "+ **Initialization:** Pick an arbitrary starting point $x_0$.\n",
    "+ For each time step $n$:\n",
    "    - **Generation:** Sample a candidate $x$ from $T(x_n, x)$.\n",
    "    - **Calculation:** Calculate the *acceptance ratio*:\n",
    "    $$\n",
    "    \\alpha(x_n, x) = \\min\\left\\{1, \\frac{h(x)}{h(x_n)}\\right\\}.\n",
    "    $$\n",
    "    This is the only place where you may need to evaluate the underlying model.\n",
    "    - **Accept/Reject:**\n",
    "        - Generate a uniform number $u\\sim \\mathcal{U}([0,1])$.\n",
    "        - If $u\\le \\alpha$, *accept* and set $x_{n+1}=x$.\n",
    "        - If $u > \\alpha$, *reject* ad set $x_{n+1} = x_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Does Metropolis Work?\n",
    "\n",
    "Well, it works because it gives us a Markov chain with the desired equilibrium distribution.\n",
    "That is, a chain that has an invariant distribution of our choice that it is also ergodic.\n",
    "\n",
    "To show that $\\pi(x)$ is the invariant distribution of the Metropolis Markov chain, we will show that the latter satisfies the detailed balance condition.\n",
    "To this end, we need the transition kernel of the chain.\n",
    "The transition kernel $K(x,x')$ gives the probability that the Metropolis chain moves from $x$ to $x'$.\n",
    "It is:\n",
    "$$\n",
    "K(x,x') = T(x,x')\\alpha(x,x') + (1 - r(x))\\delta(x' - x),\n",
    "$$\n",
    "where $T(x,x')$ is the transition kernel of the proposal distribution,\n",
    "$$\n",
    "\\alpha(x,x') = \\min\\left\\{1, \\frac{h(x')}{h(x)}\\right\\}\n",
    "$$\n",
    "is the acceptance ratio,\n",
    "$$\n",
    "r(x) = \\int T(x, y)\\alpha(x, y)dy,\n",
    "$$\n",
    "is the probability of accepting any move, i.e., $1 - r(x)$ is the probability of not accepting the move, and $\\delta(x-x')$ is the Dirac delta centered at $x'$.\n",
    "\n",
    "Let's prove that the detailed balance holds for this transition kernel.\n",
    "For $x = x'$ the equation holds trivially (even though we would have to interpret it slightly differently to be 100% rigorous).\n",
    "For $x\\not= x'$, we have:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\pi(x) K(x, x') &=& \\frac{h(x)}{Z} T(x,x')\\alpha(x,x') \\\\\n",
    "&=& \\frac{h(x)}{Z}T(x,x')\\min\\left\\{1, \\frac{h(x')}{h(x)}\\right\\}\\\\\n",
    "&=& \\frac{h(x')}{h(x')}\\frac{h(x)}{Z}T(x,x')\\min\\left\\{1, \\frac{h(x')}{h(x)}\\right\\}\\\\\n",
    "&=& \\frac{h(x')}{Z}T(x,x')\\min\\left\\{\\frac{h(x)}{h(x')},\\frac{h(x)}{h(x')}\\cdot\\frac{h(x')}{h(x)}\\right\\}\\\\\n",
    "&=& \\pi(x')T(x,x')\\min\\left\\{\\frac{h(x)}{h(x')},1\\right\\}\\\\\n",
    "&=& \\pi(x')T(x,x')\\alpha(x',x)\\\\\n",
    "&=& \\pi(x')T(x', x)\\alpha(x',x)\\\\\n",
    "&=& \\pi(x')K(x',x),\n",
    "\\end{array}\n",
    "$$\n",
    "where we also made use of the symmetry of the proposl $T(x,x') = T(x',x)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
