{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 23.1: Maximum Mean - A Bad Information Acquisition Function\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Develop intuition about the maximum mean as an information acquisition function and why you should never use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working 1D Example\n",
    "\n",
    "It is easier to introduce the ideas using an example.\n",
    "Let's work with a synthetic 1D function defined in $[0,1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 4 * (1. - np.sin(6 * x + 8 * np.exp(6 * x - 7.))) \n",
    "\n",
    "x = np.linspace(0, 1)\n",
    "plt.plot(x, f(x), linewidth=2)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to maximize this function (of course, in reality, you wouldn't see the functioin).\n",
    "Let us generate some starting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456) # For reproducibility\n",
    "n_init = 3\n",
    "X = np.random.rand(n_init) # In 1D you don't have to use LHS\n",
    "Y = f(X)\n",
    "plt.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we do some kind of Bayesian regression, using the data we have so far.\n",
    "Here, we will do GPR, but any Bayesian regression would actually work.\n",
    "We will not work right now with the full predictive $p(f(\\cdot)|\\mathcal{D}_{n})$, but with the point-predictive distribution:\n",
    "$$\n",
    "p(y|\\mathbf{x},\\mathcal{D}_{n}) = \\mathcal{N}\\left(y|m_{n}(\\mathbf{x}), \\sigma^2_{n}(\\mathbf{x})\\right),\n",
    "$$\n",
    "where $m_{n}(\\mathbf{x})$ and $\\sigma^2_{n}(\\mathbf{x})$ are the predictive mean and variance respectively.\n",
    "\n",
    "Here is an example with GPR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "# The kernel we use\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "# Assuming that we know there is no measurement noise:\n",
    "gpr.likelihood.variance.constrain_fixed(1e-16)\n",
    "# You can evaluate the predictive distribution anywhere:\n",
    "m, sigma2 = gpr.predict(x[:, None])\n",
    "# And you can visualize the results as follows\n",
    "# Standard deviation\n",
    "sigma = np.sqrt(sigma2)\n",
    "# Lower quantile\n",
    "l = m - 1.96 * sigma\n",
    "u = m + 1.96 * sigma\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "plt.plot(x, f(x), 'r--', linewidth=2, label='True function')\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='Observations')\n",
    "ax.plot(x, m, label='GP mean')\n",
    "ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25,\n",
    "                label='GP 95% pred. int.')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the question is this: \"Where should we evaluate the function next if our goal is to maximize the function?\"\n",
    "Let's start with the naive assumption that we should evaluate the function at the point that maximizes the Gaussian process posterior mean, i.e., wherever is the max of the thick blue line.\n",
    "In other words, the information acquisition function is the posterior mean of the Gaussian process.\n",
    "Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize_naive(f, gpr, X_design, max_it=6):\n",
    "    \"\"\"\n",
    "    Optimize f using a limited number of evaluations.\n",
    "    \n",
    "    :param f:        The function to optimize.\n",
    "    :param gpr:      A Gaussian process model to use for representing our state of knowldege.\n",
    "    :param X_design: The set of candidate points for identifying the maximum.\n",
    "    :param max_it:   The maximum number of iterations.\n",
    "    \"\"\"\n",
    "    for count in range(max_it):\n",
    "        m, sigma2 = gpr.predict(X_design)\n",
    "        sigma = np.sqrt(sigma2)\n",
    "        l = m - 1.96 * sigma\n",
    "        u = m + 1.96 * sigma\n",
    "        i = np.argmax(m)\n",
    "        X = np.vstack([gpr.X, X_design[i:(i+1), :]])\n",
    "        y = np.vstack([gpr.Y, [f(X_design[i, :])]])\n",
    "        gpr.set_XY(X, y)\n",
    "        fig, ax = plt.subplots(dpi=100)\n",
    "        ax.plot(gpr.X, gpr.Y, 'kx', markersize=10, markeredgewidth=2,\n",
    "                label='Observations')\n",
    "        ax.plot(x, f(x), 'r--', label='True function')\n",
    "        ax.plot(x[i], f(x[i]), 'go', label='Next observation')\n",
    "        ax.plot(x, m, label='GP mean')\n",
    "        ax.fill_between(X_design.flatten(), l.flatten(), u.flatten(), \n",
    "                        color=sns.color_palette()[0], alpha=0.25,\n",
    "                        label='GP 95% pred. int.')\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$y$')\n",
    "        ax.set_title('BGO iteration #{0:d}'.format(count+1))\n",
    "        plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the algorithm\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "maximize_naive(f, gpr, x[:, None], max_it=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the algorithm misses the real maximum.\n",
    "Instead it finds a local maximum and gets trapped by it.\n",
    "This is because using the posterior mean as an acquisition functions focuses too much on **exploiting** the current available information but fails to **explore** regions of the input space that we haven't visited.\n",
    "\n",
    "### Questions\n",
    "\n",
    "+ Experiment with different number of initial observations. How many do you have to use for the algorithm to actually converge to the global maximum?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
