{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 22.1: Gaussian Process Regression Without Noise\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Perform Gaussian process regression without measurement noise\n",
    "\n",
    "## Example: Gaussian process regression in 1D with fixed hyper-parameters\n",
    "\n",
    "Let's generate some synthetic 1D data to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the seed so that we all see the same data\n",
    "np.random.seed(1234)\n",
    "n = 10\n",
    "# The inputs are in [0, 1]\n",
    "X = np.random.rand(n, 1) # Needs to be an n x 1 vector\n",
    "# The true function that we will try to identify\n",
    "f_true = lambda x: -np.cos(np.pi * x) + np.sin(4. * np.pi * x)\n",
    "# Some data to train on\n",
    "Y = f_true(X)\n",
    "# Let's visualize the data\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will get started with the regression\n",
    "# First, import GPy\n",
    "import GPy\n",
    "# Second, pick a kernel. Let's pick a squared exponential (RBF = Radial Basis Function)\n",
    "k = GPy.kern.RBF(1) # The parameter here is the dimension of the input (here 1)\n",
    "# Let's print the kernel object to see what it includes:\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``variance`` of the kernel is one. This seems reasonable.\n",
    "Let's leave it like that.\n",
    "The ``lengthscale`` seems to big.\n",
    "Let's change it to something reasonable (based on our expectations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.lengthscale = 0.1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a possibility to choose a mean function, but for simplicity we are going to pick a zero mean function:\n",
    "$$\n",
    "m(x) = 0.\n",
    "$$\n",
    "Now we put together the GP regression model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpm = GPy.models.GPRegression(X, Y, k) # It is input, output, kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is automatically assuming that the likelihood is Gaussian (you can modify it if you wish).\n",
    "Where do can you find the $\\sigma^2$ parameter specifying the likelihood noise? Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to fix the measurement noise to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpm.likelihood.variance = 0.0\n",
    "print(gpm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. We have now specified the model completely.\n",
    "The posterior GP is completely defined.\n",
    "Where is the posterior mean $m_n(x)$ and variance $\\sigma_n^2(x)$? You can get them like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the mean on some test points\n",
    "x_star = np.linspace(0, 1, 100)[:, None] # This is needed to turn the array into a column vector\n",
    "m_star, v_star = gpm.predict(x_star)\n",
    "# Let's plot the mean first\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='data')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.plot(x_star, m_star, lw=2, label='$m_n(x)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the variance on the same test points is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the variance on the same test points\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x_star, v_star, lw=2, label='$\\sigma_n^2(x)$')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\sigma_n^2(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the variance is zero wherever we have an observation.\n",
    "\n",
    "Having the posterior mean and variance, we can derive 95\\% predictive intervals for $f(x^*)$ and $y^*$.\n",
    "For $f(x^*)$ these are:\n",
    "$$\n",
    "m_n(\\mathbf{x}^*)) - 2\\sigma_n(\\mathbf{x}^*) \\le f(\\mathbf{x}^*) \\le m_n(\\mathbf{x}^*)) + 2\\sigma_n(\\mathbf{x}^*).\n",
    "$$\n",
    "Let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "f_lower = m_star - 2.0 * np.sqrt(v_star - gpm.likelihood.variance)\n",
    "f_upper = m_star + 2.0 * np.sqrt(v_star - gpm.likelihood.variance)\n",
    "ax.fill_between(x_star.flatten(), f_lower.flatten(), f_upper.flatten(), alpha=0.5)\n",
    "ax.plot(x_star, m_star, lw=2, label='$m_n(x)$')\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='data')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also put the correct function there for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.fill_between(x_star.flatten(), f_lower.flatten(), f_upper.flatten(), alpha=0.5, label='$f(\\mathbf{x}^*)$ 95% pred.')\n",
    "ax.plot(x_star, m_star, lw=2, label='$m_n(x)$')\n",
    "ax.plot(x_star, f_true(x_star), 'm-.', label='True function')\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='data');\n",
    "#plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the true function is almost entirely within the blue bounds.\n",
    "It is ok that it is a little bit off, becuase these are 95% prediction intervals.\n",
    "About 5% of the function can be off.\n",
    "\n",
    "That's good.\n",
    "However, we have much more information encoded in the posterior GP.\n",
    "It is actually a probability measure over the space of functions.\n",
    "How do we sample functions?\n",
    "Well, you can't sample functions...\n",
    "They are infinite objects.\n",
    "But you can sample the *function values* on a bunch of test points.\n",
    "As a matter of fact, the joint probability density of the function values at any collection of set points is a multivariate Gaussian.\n",
    "We did it manually in the last lecture.\n",
    "In this lecture, we are going to use the capabilities of ``GPy``.\n",
    "Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how you take the samples\n",
    "f_post_samples = gpm.posterior_samples_f(x_star, 10) # Test points, how many samples you want\n",
    "# Here is the size of f_post_samples\n",
    "print(f_post_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is ``test points x number of outputs (1 here) x number of samples``.\n",
    "Let's plot them along with the data and the truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x_star, f_post_samples[:, 0, :], 'r', lw=0.5)\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='data');\n",
    "ax.plot(x_star, f_true(x_star), 'm-.', label='True function')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we see that the lengthscale we have assumed does not match the lengthscale of the true function perfectly.\n",
    "But that's how it is.\n",
    "In real problems, you won't know the true function anyway. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following interactive function regenerates the figures above allowing you to experiment with various choices of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual\n",
    "\n",
    "def analyze_and_plot_gp_ex1(kern_variance=1.0, kern_lengthscale=0.1):\n",
    "    \"\"\"\n",
    "    Performs GP regression with given kernel variance, lengthcale and likelihood variance.\n",
    "    \"\"\"\n",
    "    #k = GPy.kern.RBF(1)\n",
    "    k = GPy.kern.Matern32(1)\n",
    "    gp_model = GPy.models.GPRegression(X, Y, k)\n",
    "    # Set the parameters\n",
    "    gp_model.kern.variance = kern_variance\n",
    "    gp_model.kern.lengthscale = kern_lengthscale\n",
    "    gp_model.likelihood.variance = 0.0\n",
    "    # Print model for sanity check\n",
    "    print(gp_model)\n",
    "    # Pick test points\n",
    "    x_star = np.linspace(0, 1, 100)[:, None]\n",
    "    # Get posterior mean and variance\n",
    "    m_star, v_star = gp_model.predict(x_star)\n",
    "    # Plot 1: Mean and 95% predictive interval\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel('$x$')\n",
    "    ax1.set_ylabel('$y$')\n",
    "    f_lower = m_star - 2.0 * np.sqrt(v_star)\n",
    "    f_upper = m_star + 2.0 * np.sqrt(v_star)\n",
    "    ax1.fill_between(x_star.flatten(), f_lower.flatten(), f_upper.flatten(), alpha=0.5, label='$f(\\mathbf{x}^*)$ 95% pred.')\n",
    "    ax1.plot(x_star, m_star, lw=2, label='$m_n(x)$')\n",
    "    ax1.plot(x_star, f_true(x_star), 'm-.', label='True function')\n",
    "    ax1.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='data')\n",
    "    #plt.legend(loc='best');\n",
    "    \n",
    "    # Plot 2: Data plus posterior samples\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    f_post_samples = gp_model.posterior_samples_f(x_star, 10)\n",
    "    ax2.plot(x_star, f_post_samples[:, 0, :], 'r', lw=0.5)\n",
    "    ax2.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='data');\n",
    "    ax2.plot(x_star, f_true(x_star), 'm-.', label='True function')\n",
    "    ax2.set_xlabel('$x$')\n",
    "    ax2.set_ylabel('$y$')\n",
    "    #plt.legend(loc='best');\n",
    "    \n",
    "interact_manual(analyze_and_plot_gp_ex1,\n",
    "            kern_variance=(0.01, 10.0, 0.01),\n",
    "            kern_lengthscale=(0.01, 1.0, 0.01),\n",
    "            like_variance=(0.01, 1.0, 0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "In the interactive tool above:\n",
    "\n",
    "+ Experiment with differnet lengthscales for the kernel. You need to click on ``Run Interact`` for the code to run.\n",
    "What happens to the posterior mean and the 95% predictive error bar as the lengthscale increases (decreases)?\n",
    "\n",
    "+ Experiment with difference likelihood variances. What happens for very big variances? What happens for very small variances?\n",
    "\n",
    "+ Experiment with different kernel variances. This the $s^2$ parameter of the squared exponential covariance function. It specifies our prior variance about the function values. What is its effect?\n",
    "\n",
    "+ Imagine that, as it would be the case in reality, you do not know the true function. How would you pick the correct values for the hyperparameters specifying the kernel?\n",
    "\n",
    "+ Try some other kernels. Edit the function ``analyze_and_plot_gp_ex1`` and change the line ``k = GPy.kern.RBF(1)`` to ``k = GPy.kern.Matern52(1)``. This is a kernel that is less regular than the RBF. What do you observe?\n",
    "Then try ``k = GPy.kern.Matern32(1)``. Then ``k = GPy.kern.Exponential(1)``. The last one is continuous but nowhere differentiable.\n",
    "How can you pick the right kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics: How do you know if the fit is good?\n",
    "\n",
    "To objective test the resulting model we need a *validation dataset* consisting of inputs:\n",
    "$$\n",
    "\\mathbf{x}^v_{1:n^v} = \\left(\\mathbf{x}^v_1,\\dots,\\mathbf{x}^v_{n^v}\\right),\n",
    "$$\n",
    "and corresponding, observed outputs:\n",
    "$$\n",
    "\\mathbf{y}^v_{1:n^v} = \\left(y^v_1,\\dots,y^v_{n^v}\\right).\n",
    "$$\n",
    "We will use this validation dataset to define some diagnostics.\n",
    "Let's do it directly through the 1D example above.\n",
    "First, we generate some validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_v = 100\n",
    "X_v = np.random.rand(n_v)[:, None]\n",
    "Y_v = f_true(X_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point-predictions\n",
    "\n",
    "Point-predictions only use $m_n\\left(\\mathbf{x}^v_i\\right)$.\n",
    "Of course, when there is a lot of noise, they are not very useful.\n",
    "But let's look at what we get anyway.\n",
    "(In the questions section I will ask you to reduce the noise and repeat).\n",
    "\n",
    "The simplest thing we can do is to compare $y^v_i$ to $m_n\\left(\\mathbf{x}^v_i\\right)$.\n",
    "We start with the *mean square error*:\n",
    "$$\n",
    "\\operatorname{MSE} := \\frac{1}{n^v}\\sum_{i=1}^{n^v}\\left[y^v_i-m_n\\left(\\mathbf{x}^v_i\\right)\\right]^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_v, v_v = gpm.predict(X_v)\n",
    "mse = np.mean((Y_v - m_v) ** 2)\n",
    "print('MSE = {0:1.2f}'.format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not very intuitive though.\n",
    "An somewhat intuitive measure is [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination) also known as $R^2$, *R squared*.\n",
    "It is defined as:\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n^v}\\left[y_i^v - m_n(\\mathbf{x}_i^v)\\right]^2}{\\sum_{i=1}^{n^v}\\left[y_i^v-\\bar{y}^v\\right]^2},\n",
    "$$\n",
    "where $\\bar{y}^v$ is the mean of the observed data:\n",
    "$$\n",
    "\\bar{y}^v = \\frac{1}{n^v}\\sum_{i=1}^{n^v}y_i^v.\n",
    "$$\n",
    "The interpretation of $R^2$, and take this with a grain of salt, is that it gives the percentage of variance of the data explained by the model.\n",
    "A score of $R^2=1$, is a perfect fit.\n",
    "In our data we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = 1.0 - np.sum((Y_v - m_v) ** 2) / np.sum((Y_v - np.mean(Y_v)) ** 2)\n",
    "print('R2 = {0:1.2f}'.format(R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, on point-predictions, we can simply plot the predictions vs the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "y_range = np.linspace(Y_v.min(), Y_v.max(), 50)\n",
    "ax.plot(y_range, y_range, 'r', lw=2)\n",
    "ax.plot(Y_v, m_v, 'bo')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Observation');"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
