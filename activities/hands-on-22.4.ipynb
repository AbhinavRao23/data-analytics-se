{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 22.4: Multivariate Gaussian Process Regression\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Perform multivariate Gaussian process regression with automatic relevance determination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\E}{\\mathbf{E}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Gaussian Process Regression\n",
    "\n",
    "When we say \"multivariate\" here we refer to many function inputs, not outputs.\n",
    "This just means that the input of the function we are interested in learning is a vector $\\mathbf{x}$ in $\\mathbb{R}^D$.\n",
    "In that regard, nothing really changes compared to one-input GPR.\n",
    "The formulas are exactly the same.\n",
    "However, in higher dimensional settings one must pay attention to the selection of the covariance function.\n",
    "\n",
    "Let's demonstrate this using an example.\n",
    "Here is an analytical function, known as the Branin-Hoo function:\n",
    "$$\n",
    "f(\\mathbf{x}) = f(x_1,x_2) = \\frac{1}{51.95}\\left[\\left(15x_2 - \\frac{5.1(15x_1)^2}{4\\pi^2} + \\frac{75x_1}{\\pi} - 6\\right)^2 + \\left(10 - \\frac{10}{8\\pi}\\right)\\cos(15x_1)-44.81\\right]\n",
    "$$\n",
    "Here $\\mathbf{x} = (x_1, x_2)$ and it takes values in $[0,1]^2$.\n",
    "We will use this function to generate some synthetic data.\n",
    "Let's superimpose the synthetic data on a contour of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f_branin(x):\n",
    "    \"\"\"\n",
    "    Input must be 2D array of size N x 2.\n",
    "    \"\"\"\n",
    "    return 1.0 / 51.95 * ((15.0 * x[:, 1] - 5.1 * (15.0 * x[:, 0]) ** 2 / 4.0 / np.pi ** 2 + \n",
    "                          75.0 * x[:, 0] / np.pi - 6.0) ** 2\n",
    "                         + (10.0 - 10.0 / 8.0 / np.pi) * np.cos(15.0 * x[:, 0]) - 44.81)\n",
    "\n",
    "# Generate synthetic data\n",
    "N = 30\n",
    "Xdata = np.random.rand(N, 2)\n",
    "sigma = 0.01\n",
    "Ydata = f_branin(Xdata)[:, None] + sigma * np.random.randn(N, 1)\n",
    "\n",
    "# Plot the contour and the observations\n",
    "fig = plt.figure( figsize=(12, 8) )\n",
    "ax = plt.subplot(111)\n",
    "x1 = np.linspace(0, 1, 100)\n",
    "x2 = np.linspace(0, 1, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "_Xs = np.vstack([X1.flatten(), X2.flatten()]).T\n",
    "Y = f_branin(_Xs).reshape((100, 100))\n",
    "\n",
    "c = ax.contourf(X1, X2, Y, 30, cmap='plasma')\n",
    "plt.colorbar(c)\n",
    "ax.plot(Xdata[:, 0], Xdata[:, 1], 'o', color='black',  markersize=10)\n",
    "ax.tick_params(labelsize=20)\n",
    "\n",
    "ax.set_xlabel('$x_1$', fontsize=15)\n",
    "ax.set_ylabel('$x_2$', fontsize=15)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a the squared exponential covariance function for a regression along with a zero mean.\n",
    "So our prior GP is:\n",
    "$$\n",
    "f(\\cdot)\\sim\\operatorname{GP}(0, k(\\cdot,\\cdot)),\n",
    "$$\n",
    "with\n",
    "$$\n",
    "k(\\mathbf{x}, \\mathbf{x}') = s^2\\exp\\left(-\\frac{(x_1-x_1')^2}{2\\ell_1^2}-\\frac{(x_2-x_2')^2}{2\\ell_2^2}\\right).\n",
    "$$\n",
    "The first, situation we are going to examin is when the lengthscales are the same:\n",
    "$$\n",
    "\\ell_1 = \\ell_2.\n",
    "$$\n",
    "This is what you get by default from ``GPy``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "k = GPy.kern.RBF(2) # The 2 here is for the dimensionality of the input space\n",
    "gp = GPy.models.GPRegression(Xdata, Ydata, k)\n",
    "print(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the optimization \n",
    "gp.optimize(messages=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the model details \n",
    "print(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the lengthscale \n",
    "print(gp.kern.lengthscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "Ypred_flat, Yvar_flat = gp.predict( _Xs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start a new figure\n",
    "fig = plt.figure( figsize=(12, 10) )\n",
    "\n",
    "#plot the contour of the predictions (Compare to contour of true values above)\n",
    "ax = plt.subplot(221)\n",
    "ax.tick_params(axis='both', labelsize=15)\n",
    "Ypred = Ypred_flat.reshape( (100, 100) )\n",
    "im = ax.contourf(X1, X2, Ypred, 30, cmap='plasma')\n",
    "c = plt.colorbar(im, ax=ax)\n",
    "c.ax.tick_params(labelsize=12)\n",
    "ax.plot(Xdata[:, 0], Xdata[:, 1], 'o', color='black', markersize=8)\n",
    "ax.set_xlabel('$x_1$', fontsize=15)\n",
    "ax.set_ylabel('$x_2$', fontsize=15)\n",
    "ax.set_title('Predictions', fontsize=15)\n",
    "\n",
    "# Let's also plot the contour of the error\n",
    "ax = plt.subplot(222)\n",
    "ax.tick_params(axis='both', labelsize=15)\n",
    "E = np.abs(Y - Ypred)\n",
    "ax.plot(Xdata[:, 0], Xdata[:, 1], 'o', color='black', markersize=8)\n",
    "im = ax.contourf(X1, X2, E, 30, cmap='plasma')\n",
    "c = plt.colorbar(im, ax=ax)\n",
    "c.ax.tick_params(labelsize=15)\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "ax.set_title('Error', fontsize=12)\n",
    "\n",
    "# And let's compare the error plot to 2 x the predictive standard deviation\n",
    "ax = plt.subplot(212)\n",
    "ax.tick_params(axis='both', labelsize=15)\n",
    "\n",
    "im = ax.contourf(X1, X2, 2.0 * np.sqrt(Yvar_flat.reshape(X1.shape)), 30, cmap='plasma')\n",
    "ax.set_title('Uncertainty', fontsize=12)\n",
    "c = plt.colorbar(im, ax=ax)\n",
    "c.ax.tick_params(labelsize=15)\n",
    "\n",
    "ax.plot(Xdata[:, 0], Xdata[:, 1], 'o', color='black', markersize=8)\n",
    "ax.set_xlabel('$x_1$', fontsize=12)\n",
    "ax.set_ylabel('$x_2$', fontsize=12)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some test data and make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "Ntest = 100\n",
    "Xtest = np.random.rand(Ntest, 2)\n",
    "Ytest = f_branin(Xtest)[:, None] \n",
    "Ytest_m, Ytest_v = gp.predict(Xtest, full_cov=False) \n",
    "error_m = np.sqrt( mean_squared_error(Ytest, Ytest_m) )\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "ax  = plt.subplot(111)\n",
    "\n",
    "_x = np.linspace( np.min(Ytest), np.max(Ytest), 100 )\n",
    "ax.plot(_x, _x, 'r--', label='$x=y$')\n",
    "ax.plot(Ytest, Ytest_m, 'o')\n",
    "ax.set_title('Predictions vs True test values', fontsize=15)\n",
    "ax.set_xlabel('Test labels', fontsize=15)\n",
    "ax.set_ylabel('Predictions', fontsize=15)\n",
    "ax.tick_params( labelsize=15 )\n",
    "\n",
    "fig.tight_layout()\n",
    "print(error_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's repeat the same analysis but allowing for a different lengthscale per input.\n",
    "First, we set up and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = GPy.kern.RBF(2, ARD=True) # The ARD = True is what makes GPy understand that there is\n",
    "                              # one lengthscale per dimension\n",
    "gp = GPy.models.GPRegression(Xdata, Ydata, k)\n",
    "print(gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you now cannot see what the lengthscale values are.\n",
    "You need to do this to see them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can train the model as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.optimize( messages=True )\n",
    "print(gp)\n",
    "print(gp.rbf.lengthscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you see that the lengthscale assigned to $x_1$ is slightly bigger than the lengthscale assigned to $x_2$. This means that the first input is slightly more important than the second input.\n",
    "This is the automatic relevance determination (ARD) in action.\n",
    "\n",
    "Now let's try to make some predictions with the ARD version of the GP regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntest = 100\n",
    "Xtest = np.random.rand(Ntest, 2)\n",
    "Ytest = f_branin(Xtest)[:, None] \n",
    "Ytest_m, Ytest_v = gp.predict(Xtest, full_cov=False) \n",
    "error_m = np.sqrt( mean_squared_error(Ytest, Ytest_m) )\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "ax  = plt.subplot(111)\n",
    "\n",
    "_x = np.linspace( np.min(Ytest), np.max(Ytest), 100 )\n",
    "ax.plot(_x, _x, 'r--', label='$x=y$')\n",
    "ax.plot(Ytest, Ytest_m, 'o')\n",
    "ax.set_title('Predictions vs True test values', fontsize=15)\n",
    "ax.set_xlabel('Test labels', fontsize=15)\n",
    "ax.set_ylabel('Predictions', fontsize=15)\n",
    "ax.tick_params( labelsize=15 )\n",
    "\n",
    "fig.tight_layout()\n",
    "print(error_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a third dimension to the Branin function, $x_3$, such that $f$ has a weak linear dependence on $x_3$. \n",
    "\n",
    "The added term is highlighted in blue.\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = f(x_1,x_2, x_3) = \\frac{1}{51.95}\\left[\\left(15x_2 - \\frac{5.1(15x_1)^2}{4\\pi^2} + \\frac{75x_1}{\\pi} - 6\\right)^2 + \\left(10 - \\frac{10}{8\\pi}\\right)\\cos(15x_1)-44.81\\right] + \\color{blue}{0.1 x_3}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f_branin_1(x):\n",
    "    \"\"\"\n",
    "    Input must be 2D array of size N x 2.\n",
    "    \"\"\"\n",
    "    return (1.0 / 51.95 * ((15.0 * x[:, 1] - 5.1 * (15.0 * x[:, 0]) ** 2 / 4.0 / np.pi ** 2 + \n",
    "                          75.0 * x[:, 0] / np.pi - 6.0) ** 2\n",
    "                         + (10.0 - 10.0 / 8.0 / np.pi) * np.cos(15.0 * x[:, 0]) - 44.81)) + (0.1*x[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some synthetic data and train a GP model with ARD. Let's see if we can discover the weak dependence of $f$ on the newly added 3rd dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "N    = 40\n",
    "ndim = 3\n",
    "Xdata = np.random.rand(N, ndim)\n",
    "\n",
    "# We will also be adding a little bit of noise to the observations\n",
    "sigma = 0.01\n",
    "Ydata = f_branin_1(Xdata)[:, None] + sigma * np.random.randn(N, 1)\n",
    "\n",
    "k = GPy.kern.RBF(ndim, ARD=True) \n",
    "gp = GPy.models.GPRegression(Xdata, Ydata, k)\n",
    "print(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the GP\n",
    "gp.optimize(messages=True); # No messages shown here\n",
    "print(gp.rbf.lengthscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the results above that the lengthscale in the 3rd dimension is very large relative to the lengthscales along the 1st and 2nd dimensions. \n",
    "This means that our trained GP model depends very weakly on $x_3$. Clearly, we are able to capture the weak dependence of $f$ on $x_3$. \n",
    "\n",
    "Let's make some predictions on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntest = 100\n",
    "Xtest = np.random.rand(Ntest, ndim)\n",
    "Ytest = f_branin_1(Xtest)[:, None] \n",
    "Ytest_m, Ytest_v = gp.predict(Xtest, full_cov=False) \n",
    "error_m = np.sqrt( mean_squared_error(Ytest, Ytest_m) )\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "ax  = plt.subplot(111)\n",
    "\n",
    "_x = np.linspace( np.min(Ytest), np.max(Ytest), 100 )\n",
    "ax.plot(_x, _x, 'r--', label='$x=y$')\n",
    "ax.plot(Ytest, Ytest_m, 'o')\n",
    "ax.set_title('Predictions vs True test values', fontsize=15)\n",
    "ax.set_xlabel('Test labels', fontsize=15)\n",
    "ax.set_ylabel('Predictions', fontsize=15)\n",
    "ax.tick_params( labelsize=15 )\n",
    "\n",
    "fig.tight_layout()\n",
    "print(error_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a fourth dimension to the Branin function, $x_4$, such that $f(x_1, x_2, x_3, x_4) = f(x_1, x_2, x_3)$, i.e., $f$ does not actually depend on $x_4$. \n",
    "\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = f(x_1,x_2, x_3, x_4) = \\frac{1}{51.95}\\left[\\left(15x_2 - \\frac{5.1(15x_1)^2}{4\\pi^2} + \\frac{75x_1}{\\pi} - 6\\right)^2 + \\left(10 - \\frac{10}{8\\pi}\\right)\\cos(15x_1)-44.81\\right] + \\color{blue}{0.1 x_3}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def f_branin_2(x):\n",
    "    assert x.shape[1] == 4, 'The input should be 4 dimensional.'\n",
    "    return f_branin_1(x[:, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "N    = 40\n",
    "ndim = 4\n",
    "Xdata = np.random.rand(N, ndim)\n",
    "\n",
    "# We will also be adding a little bit of noise to the observations\n",
    "sigma = 0.01\n",
    "Ydata = f_branin_2(Xdata)[:, None] + sigma * np.random.randn(N, 1)\n",
    "\n",
    "k = GPy.kern.RBF(ndim, ARD=True) \n",
    "gp = GPy.models.GPRegression(Xdata, Ydata, k)\n",
    "print(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the GP\n",
    "gp.optimize(messages=True); # No messages shown here\n",
    "print(gp.rbf.lengthscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $\\ell_4$ is much larger than $\\ell_3$, which itself is much larger than $\\ell_1$ and $\\ell_2$. \n",
    "The ARD training procedure discovers that the dependence of the function $f$ is very weak in the input $x_4$.\n",
    "\n",
    "Now let's make some predictions and compare it to the test observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntest = 100\n",
    "Xtest = np.random.rand(Ntest, ndim)\n",
    "Ytest = f_branin_2(Xtest)[:, None] \n",
    "Ytest_m, Ytest_v = gp.predict(Xtest, full_cov=False) \n",
    "error_m = np.sqrt( mean_squared_error(Ytest, Ytest_m) )\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "ax  = plt.subplot(111)\n",
    "\n",
    "_x = np.linspace( np.min(Ytest), np.max(Ytest), 100 )\n",
    "ax.plot(_x, _x, 'r--', label='$x=y$')\n",
    "ax.plot(Ytest, Ytest_m, 'o')\n",
    "ax.set_title('Predictions vs True test values', fontsize=15)\n",
    "ax.set_xlabel('Test labels', fontsize=15)\n",
    "ax.set_ylabel('Predictions', fontsize=15)\n",
    "ax.tick_params( labelsize=15 )\n",
    "\n",
    "fig.tight_layout()\n",
    "print(error_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "For the very last example (4D):\n",
    "+ Add code cells above that compute the standarized errors and does the quantile-quantile plots.\n",
    "+ Experiment with very small number of samples $N$. What happens?\n",
    "+ Experiment with very large number of samples, say $N = 500$. What happens?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
