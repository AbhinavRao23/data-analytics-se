{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 23.5: Expected Improvement - With Observation Noise\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Develop intuition about the expected improvement in the presence of observation noise\n",
    "\n",
    "## Optimizing Noisy Functions\n",
    "\n",
    "The optimization of noisy functions is relevant when you are dealing with experimentally measured objectives.\n",
    "In such a scenario, you do not observe $f(\\mathbf{x})$, but a noisy version of it.\n",
    "Here is a prototypical scenario of an experimentally measured objective.\n",
    "Let $\\xi$ be all the variables that affect the objective and assume that they are distributed in a way, not necessarily known to you:\n",
    "$$\n",
    "\\xi \\sim p(\\xi).\n",
    "$$\n",
    "The you setup your experiment using a design $\\mathbf{x}$ and you measure:\n",
    "$$\n",
    "y = g(\\mathbf{x}, \\xi).\n",
    "$$\n",
    "Let's assume now, that you would like to maximize the expectation of this function, i.e., you want to maximize\n",
    "$$\n",
    "f(\\mathbf{x}) = \\mathbb{E}_\\xi[g(\\mathbf{x},\\xi)].\n",
    "$$\n",
    "The expectation here is over the experimental noise.\n",
    "\n",
    "A naÃ¯ve way of solving this problem is to approximate the expectation using sample averaging.\n",
    "That is, at each $\\mathbf{x}$ you do many experiments instead of just one.\n",
    "If your experiments are not very expensive, you may just do that.\n",
    "Then any of the algorithms above would work in your problem.\n",
    "\n",
    "So, what do you do when you cannot get rid of the noise?\n",
    "Well, this is an open problem.\n",
    "But here is a quick and dirty solution which may work in many cases.\n",
    "First, use GPR to approximate $f(\\mathbf{x})$ using noisy measurements:\n",
    "$$\n",
    "y_i = g(\\mathbf{x},\\xi_i),\n",
    "$$\n",
    "for $i=1,\\dots,n$. \n",
    "Note that here you do not necessarily have to observe the $\\xi$'s.\n",
    "You can assume that they are hidden. If you do observe them, you can exploit this fact.\n",
    "But, for now, let's assume that you don't observe them.\n",
    "If you don't observe them you need to somehow model their effect.\n",
    "The easier thing to do is to assume that their effect is additive, zero mean, and Gaussian.\n",
    "That is, we just assume that:\n",
    "$$\n",
    "y_i = f(\\mathbf{x}_i) + \\epsilon_i,\n",
    "$$\n",
    "where $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$, where $\\sigma^2$ is to be determined.\n",
    "Of course, this is, in general, not the right assumption for the noise.\n",
    "It is, however, the easiest thing to do.\n",
    "When we do this, we can just use GPR to find our posterior state of knowledge about $f(\\cdot)$ and any hyperparameters).\n",
    "We will be just using a MAP estimate for the hyperparameters.\n",
    "\n",
    "The posterior GP conditioned on the observed data is given by the usual formulas:\n",
    "$$\n",
    "f(\\cdot)|\\mathcal{D}_n \\sim \\operatorname{GP}(m_n(\\cdot), k_n(\\cdot,\\cdot)).\n",
    "$$\n",
    "Since we will be doing sequential experiment design, we are also going to need the point-predictive distribution for the measurement $y$ at a hypothetical $\\mathbf{x}$.\n",
    "It is:\n",
    "$$\n",
    "p(y|\\mathbf{x},\\mathcal{D}_n) = \\mathcal{N}(y|m_n(\\mathbf{x}), \\sigma_n^2(\\mathbf{x}) + \\sigma^2),\n",
    "$$\n",
    "where $\\sigma_n^2(\\mathbf{x}) = k_n(\\mathbf{x},\\mathbf{x})$, i.e., the posterior variance for $f(\\mathbf{x})$.\n",
    "We now have all the ingredients to modify the information acquisition functions we had above for the noisy cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a working example let's generate data with noise\n",
    "np.random.seed(123456) # For reproducibility\n",
    "# The noise\n",
    "sigma_noise = 0.4\n",
    "# Here is the underlying function without any noise\n",
    "def f(x):\n",
    "    return 4 * (1. - np.sin(6 * x + 8 * np.exp(6 * x - 7.))) \n",
    "\n",
    "# Here is a noisy function\n",
    "def g(x):\n",
    "    return f(x) + sigma_noise * np.random.randn(x.shape[0])\n",
    "\n",
    "n_init = 4\n",
    "X = np.random.rand(n_init) # In 1D you don't have to use LHS\n",
    "Y = g(X)\n",
    "\n",
    "# The kernel we use\n",
    "import GPy\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "# Assuming that we know the measurement noise (optimize otherwise, but you will need some data)\n",
    "gpr.likelihood.variance.constrain_fixed(sigma_noise ** 2)\n",
    "# You can evaluate the predictive distribution anywhere:\n",
    "x = np.linspace(0, 1, 100)\n",
    "mn, v = gpr.predict(x[:, None])\n",
    "sigman2 = v - sigma_noise ** 2\n",
    "# And you can visualize the results as follows\n",
    "# Standard deviation\n",
    "sigman = np.sqrt(sigman2)\n",
    "# Lower quantile\n",
    "l = mn - 1.96 * sigman\n",
    "u = mn + 1.96 * sigman\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.plot(x, mn)\n",
    "ax.fill_between(x, l.flatten() - 2.0 * sigma_noise, u.flatten() + 2.0 * sigma_noise, color=sns.color_palette()[1], alpha=0.25);\n",
    "ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum upper interval with noise\n",
    "\n",
    "The maximum upper interval remains the same:\n",
    "$$\n",
    "a_n(\\mathbf{x}) = \\mu_n(\\mathbf{x}) + \\psi \\sigma_n(\\mathbf{x}),\n",
    "$$\n",
    "for some $\\psi \\ge 0$.\n",
    "Just make sure you don't add the $\\sigma^2$ term in the variance.\n",
    "Of course, we need to modify the way we grab the variance from ``GPy`` for this.\n",
    "Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mui(m, sigma, ymax, psi=1.96):\n",
    "    return m + psi * sigma\n",
    "\n",
    "def plot_mui_w_noise(psi=0.):\n",
    "    fig, ax = plt.subplots(dpi=100)\n",
    "    ax.set_title('$\\psi={0:1.2f}$'.format(psi))\n",
    "    ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.plot(x, mn)\n",
    "    ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25)\n",
    "    af_values = mui(mn, sigman, Y.max(), psi)\n",
    "    next_id = np.argmax(af_values)\n",
    "    next_x = x[next_id]\n",
    "    af_max = af_values[next_id]\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x, af_values, color=sns.color_palette()[1])\n",
    "    ax2.set_ylabel('Maximum Upper Interval', color=sns.color_palette()[1])\n",
    "    plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "    ax2.plot(next_x * np.ones(100), np.linspace(0, af_max, 100), color=sns.color_palette()[1],\n",
    "         linewidth=1)\n",
    "\n",
    "from ipywidgets import interactive\n",
    "\n",
    "interactive(plot_mui_w_noise, psi=(0., 4., 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of improvement with noise\n",
    "\n",
    "We need to make some modifications to the probability of improvement.\n",
    "First, we cannot just use the $y_n^*$ as the maximum of the $y_i$'s, because they are noisy.\n",
    "Instead, we are looking at the predictive mean of the GP, $m_n(\\mathbf{x}_i)$, at the corresponding inputs, $\\mathbf{x}_i$, and we find their maximum.\n",
    "So, define:\n",
    "$$\n",
    "m_n^* = \\max_{1\\le i\\le n}m_n(\\mathbf{x}_i).\n",
    "$$\n",
    "Essentially, instead of finding the maximum of the noisy observations, we are smoothing with the predictive mean of the GP and find the maximum of the smoothed versions.\n",
    "The rest is similar.\n",
    "The acquisition function is defined by:\n",
    "$$\n",
    "a_n(\\mathbf{x}) = \\mathbb{P}[f(\\mathbf{x}) > m_n^* + \\psi | \\mathbf{x}, \\mathcal{D}_n].\n",
    "$$\n",
    "We read \"$a_n(\\mathbf{x})$\" is the probability that we observe at $x$ a $y$ that is greater than the currently observed maximum $y_n^*$ by at least $\\psi>0$.\n",
    "The good thing is that it is possible to get an analytical answer because our point predictive distribution is Gaussian.\n",
    "In particular, we get:\n",
    "$$\n",
    "\\begin{align}\n",
    "a_n(\\mathbf{x}) &=& \\mathbb{P}[f(\\mathbf{x}) > m_n^* + \\psi | \\mathbf{x}, \\mathcal{D}_n]\\\\\n",
    "&=& \\mathbb{P}\\left[\\frac{f(\\mathbf{x}) - \\mu_n(\\mathbf{x})}{\\sigma_n(\\mathbf{x})} > \\frac{m_n^* + \\psi - \\mu_n(\\mathbf{x})}{\\sigma_n(\\mathbf{x})} \\Big| \\mathbf{x}, \\mathcal{D}_n\\right]\\\\\n",
    "&=& \\dots\\\\\n",
    "&=& \\Phi\\left(\\frac{\\mu_n(\\mathbf{x}) - m_n^* - \\psi}{\\sigma_n(\\mathbf{x})} \\right).\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poi(m, sigma, ymax, psi=0.):\n",
    "    \"\"\"\n",
    "    Return the probability of improvement.\n",
    "    \n",
    "    Arguments:\n",
    "    m        -      the predictive mean at the test points.\n",
    "    sigma        -  the predictive standard deviation at the test points.\n",
    "    ymax     -      the maximum observed value (so far).\n",
    "    psi      -      a parameter that controls exploration.\n",
    "    \"\"\"\n",
    "    return st.norm.cdf((m - ymax - psi) / sigma)\n",
    "\n",
    "# We just need to get the smoothed versions of the observed y's\n",
    "y_smoothed = gpr.predict(X[:, None])[0]\n",
    "# and then their maximum\n",
    "mn_star = y_smoothed.max()\n",
    "\n",
    "def plot_poi_w_noise(psi=0.):\n",
    "    fig, ax = plt.subplots(dpi=100)\n",
    "    ax.set_title('$\\psi={0:1.2f}$'.format(psi))\n",
    "    ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.plot(x, mn)\n",
    "    ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25)\n",
    "    af_values = poi(mn, sigman, mn_star, psi)\n",
    "    next_id = np.argmax(af_values)\n",
    "    next_x = x[next_id]\n",
    "    af_max = af_values[next_id]\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x, af_values, color=sns.color_palette()[1])\n",
    "    ax2.set_ylabel('Probability of Improvement', color=sns.color_palette()[1])\n",
    "    plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "    ax2.plot(next_x * np.ones(100), np.linspace(0, af_max, 100), \n",
    "             color=sns.color_palette()[1], linewidth=1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "interactive(plot_poi_w_noise, psi=(0., 4., 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected improvment with noise\n",
    "\n",
    "The arguments here are as in the previous section about the probability of improvement.\n",
    "Consider a hypothetical experiment at $\\mathbf{x}$ and assume that you observed $y$.\n",
    "How much improvement is that compared to your currently best observed point $m_n^*$.\n",
    "It is:\n",
    "$$\n",
    "I_n(\\mathbf{x}, f(\\mathbf{x})) =\n",
    "\\begin{cases}\n",
    "0,&\\;\\text{if}\\;f(\\mathbf{x}) \\le m_n^*,\\\\\n",
    "f(\\mathbf{x}) - m_n^*,&\\;\\text{otherwise},\n",
    "\\end{cases}\n",
    "$$\n",
    "and this conditional of $y$, i.e., $f(\\mathbf{x})$ is a random variable conditioned on $y$.\n",
    "Taking the expectation over $y$ would give you:\n",
    "$$\n",
    "\\operatorname{EI}_n(\\mathbf{x}) = \\frac{m_n(\\mathbf{x}) - m_n^*}{\\sigma_n(\\mathbf{x})}\\Phi\\left(\\frac{m_n(\\mathbf{x}) - m_n^*}{\\sigma_n(\\mathbf{x})}\\right)\n",
    "+ \\phi\\left(\\frac{m_n(\\mathbf{x}) - m_n^*}{\\sigma_n(\\mathbf{x})}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ei(m, sigma, ymax, psi=0.):\n",
    "    u = (m - ymax) / sigma\n",
    "    ei = sigma * (u * st.norm.cdf(u) + st.norm.pdf(u))\n",
    "    ei[sigma <= 0.] = 0.\n",
    "    return ei\n",
    "\n",
    "af_values = ei(mn, sigman, mn_star)\n",
    "idx = np.argmax(af_values)\n",
    "af_max = af_values[idx]\n",
    "next_x = x[idx]\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.plot(x, mn)\n",
    "ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x, af_values, color=sns.color_palette()[1])\n",
    "plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "ax2.set_ylabel('Expected Improvement', color=sns.color_palette()[1])\n",
    "ax2.plot(next_x * np.ones(100), np.linspace(0, af_max, 100), color=sns.color_palette()[1],\n",
    "         linewidth=1)\n",
    "ax2.set_ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify now our sequential information acquisition algorithm so that it works with noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize_w_noise(g, gpr, X_design, alpha=ei, psi=0., max_it=6):\n",
    "    \"\"\"\n",
    "    Optimize f using a limited number of evaluations.\n",
    "    \n",
    "    :param f:        The function to optimize.\n",
    "    :param gpr:      A Gaussian process model to use for representing our state of knowldege.\n",
    "    :param X_design: The set of candidate points for identifying the maximum.\n",
    "    :param alpha:    The acquisition function.\n",
    "    :param psi:      The parameter value for the acquisition function (not used for EI).\n",
    "    :param max_it:   The maximum number of iterations.\n",
    "    \"\"\"\n",
    "    af_all = []\n",
    "    for count in range(max_it):\n",
    "        mn, v = gpr.predict(X_design)\n",
    "        sigman2 = v - gpr.likelihood.variance\n",
    "        sigman = np.sqrt(sigman2)\n",
    "        l = mn - 1.96 * sigman\n",
    "        u = mn + 1.96 * sigman\n",
    "        y_smoothed = gpr.predict(gpr.X)[0]\n",
    "        mn_star = y_smoothed.max()\n",
    "        af_values = alpha(mn, sigman, mn_star, psi=psi)\n",
    "        i = np.argmax(af_values)\n",
    "        X = np.vstack([gpr.X, X_design[i:(i+1), :]])\n",
    "        y = np.vstack([gpr.Y, [g(X_design[i, :])]])\n",
    "        gpr.set_XY(X, y)\n",
    "        # Uncomment the following to optimize the hyper-parameters\n",
    "        # gpr.optimize()\n",
    "        af_all.append(af_values[i])\n",
    "        fig, ax = plt.subplots(dpi=100)\n",
    "        ax.plot(gpr.X, gpr.Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$y$')\n",
    "        ax.plot(x, mn)\n",
    "        ax.fill_between(X_design.flatten(), l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25)\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(X_design, af_values, color=sns.color_palette()[1])\n",
    "        plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "        ax2.set_ylabel('acquisition function', color=sns.color_palette()[1])\n",
    "        ax2.plot(X_design[i, :] * np.ones(100), np.linspace(0, af_values[i], 100), color=sns.color_palette()[1],\n",
    "                 linewidth=1)\n",
    "    return af_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a working example let's generate data with noise\n",
    "np.random.seed(123456) # For reproducibility\n",
    "# The noise\n",
    "sigma_noise = 0.4\n",
    "# Here is a noisy function\n",
    "def g(x):\n",
    "    return f(x) + sigma_noise * np.random.randn(x.shape[0])\n",
    "\n",
    "n_init = 4\n",
    "X = np.random.rand(n_init) # In 1D you don't have to use LHS\n",
    "Y = g(X)\n",
    "\n",
    "# Prepare the initial statistical model\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "gpr.likelihood.variance.constrain_fixed(sigma_noise ** 2)\n",
    "# Run the algorithm\n",
    "af_all = maximize_w_noise(g, gpr, x[:, None], alpha=ei, psi=0., max_it=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "+ Rerun the algorithm above starting with 3 observations. Does the algorithm find the same local maximum?\n",
    "If not, why?\n",
    "+ Experiment with smaller noise. \n",
    "+ Experiment with larger noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
