{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 23.4: Expected Improvement\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Develop intuition about the expected improvement\n",
    "\n",
    "## Expected Improvement\n",
    "Let's reintroduce the same running example as the previous hands-on activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 4 * (1. - np.sin(6 * x + 8 * np.exp(6 * x - 7.))) \n",
    "\n",
    "np.random.seed(123456) # For reproducibility\n",
    "n_init = 3\n",
    "X = np.random.rand(n_init) # In 1D you don't have to use LHS\n",
    "Y = f(X)\n",
    "plt.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "x = np.linspace(0, 1)\n",
    "plt.plot(x, f(x), linewidth=2)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the previous hands-on activity, assume that we have made some observations and that we have used them to do Gaussian process regression resulting in the point-predictive distribution:\n",
    "$$\n",
    "p(y|\\mathbf{x},\\mathcal{D}_{n}) = \\mathcal{N}\\left(y|m_{n}(\\mathbf{x}), \\sigma^2_{n}(\\mathbf{x})\\right),\n",
    "$$\n",
    "where $m_{n}(\\mathbf{x})$ and $\\sigma^2_{n}(\\mathbf{x})$ are the predictive mean and variance respectively.\n",
    "Here is the code for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "# The kernel we use\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "# Assuming that we know there is no measurement noise:\n",
    "gpr.likelihood.variance.constrain_fixed(1e-16)\n",
    "# You can evaluate the predictive distribution anywhere:\n",
    "m, sigma2 = gpr.predict(x[:, None])\n",
    "# And you can visualize the results as follows\n",
    "# Standard deviation\n",
    "sigma = np.sqrt(sigma2)\n",
    "# Lower quantile\n",
    "l = m - 1.96 * sigma\n",
    "u = m + 1.96 * sigma\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "plt.plot(x, f(x), 'r--', linewidth=2, label='True function')\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='Observations')\n",
    "ax.plot(x, m, label='GP mean')\n",
    "ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25,\n",
    "                label='GP 95% pred. int.')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected improvement is a bit more involved, but it serves as a template for deriving more general information acquisition functions.\n",
    "Here is how you think.\n",
    "Consider a hypothetical experiment at $\\mathbf{x}$ and assume that you observed $y$.\n",
    "How much improvement is that compared to your currently best observed point $y_n^*$.\n",
    "It is:\n",
    "$$\n",
    "I_n(\\mathbf{x}, y) =\n",
    "\\begin{cases}\n",
    "0,&\\;\\text{if}\\;y \\le y_n^*,\\\\\n",
    "y - y_n^*,&\\;\\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "But you don't know what $y$ is. What do you do now?\n",
    "Well, the only legitimate thing to do is to take the expectation over what you expected $y$ to be given what you know at that moint.\n",
    "So it is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\operatorname{EI}_n(\\mathbf{x}) &=& \n",
    "\\int_{-\\infty}^\\infty I_n(\\mathbf{x}, y)p(y|\\mathbf{x}, \\mathcal{D}_n)dy\\\\\n",
    "&=& \\int_{-\\infty}^{y_n^*}0\\cdot p(y|\\mathbf{x}, \\mathcal{D}_n)dy\n",
    "+ \\int_{y_n^*}^{\\infty}(y - y_n^*)\\cdot p(y|\\mathbf{x}, \\mathcal{D}_n)dy.\n",
    "\\end{align}\n",
    "$$\n",
    "You can work this out analytically.\n",
    "You will get:\n",
    "$$\n",
    "\\operatorname{EI}_n(\\mathbf{x}) = \\frac{m_n(\\mathbf{x}) - y_n^*}{\\sigma_n(\\mathbf{x})}\\Phi\\left(\\frac{m_n(\\mathbf{x}) - y_n^*}{\\sigma_n(\\mathbf{x})}\\right)\n",
    "+ \\phi\\left(\\frac{m_n(\\mathbf{x}) - y_n^*}{\\sigma_n(\\mathbf{x})}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ei(m, sigma, ymax, psi=0.):\n",
    "    u = (m - ymax) / sigma\n",
    "    ei = sigma * (u * st.norm.cdf(u) + st.norm.pdf(u))\n",
    "    ei[sigma <= 0.] = 0.\n",
    "    return ei\n",
    "\n",
    "af_values = ei(m, sigma, Y.max())\n",
    "idx = np.argmax(af_values)\n",
    "af_max = af_values[idx]\n",
    "next_x = x[idx]\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.plot(x, m)\n",
    "ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x, af_values, color=sns.color_palette()[1])\n",
    "plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "ax2.set_ylabel('Expected Improvement', color=sns.color_palette()[1])\n",
    "ax2.plot(next_x * np.ones(100), np.linspace(0, af_max, 100), color=sns.color_palette()[1],\n",
    "         linewidth=1)\n",
    "ax2.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian global optimization with the expected improvement\n",
    "\n",
    "Let's now run the Bayesian global optimization algorithm using the expected improvement as the information acquisition function.\n",
    "Here is the generic code from the previous hands-on activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize(f, gpr, X_design, alpha, psi=0., max_it=6):\n",
    "    \"\"\"\n",
    "    Optimize f using a limited number of evaluations.\n",
    "    \n",
    "    :param f:        The function to optimize.\n",
    "    :param gpr:      A Gaussian process model to use for representing our state of knowldege.\n",
    "    :param X_design: The set of candidate points for identifying the maximum.\n",
    "    :param alpha:    The acquisition function.\n",
    "    :param psi:      The parameter value for the acquisition function (not used for EI).\n",
    "    :param max_it:   The maximum number of iterations.\n",
    "    \"\"\"\n",
    "    af_all = []\n",
    "    for count in range(max_it):\n",
    "        m, sigma2 = gpr.predict(X_design)\n",
    "        sigma = np.sqrt(sigma2)\n",
    "        l = m - 1.96 * sigma\n",
    "        u = m + 1.96 * sigma\n",
    "        af_values = alpha(m, sigma, gpr.Y.max(), psi=psi)\n",
    "        i = np.argmax(af_values)\n",
    "        X = np.vstack([gpr.X, X_design[i:(i+1), :]])\n",
    "        y = np.vstack([gpr.Y, [f(X_design[i, :])]])\n",
    "        gpr.set_XY(X, y)\n",
    "        # Uncomment the following to optimize the hyper-parameters\n",
    "        # gpr.optimize()\n",
    "        af_all.append(af_values[i])\n",
    "        fig, ax = plt.subplots(dpi=100)\n",
    "        ax.plot(gpr.X, gpr.Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$y$')\n",
    "        ax.plot(x, m)\n",
    "        ax.fill_between(X_design.flatten(), l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25)\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(X_design, af_values, color=sns.color_palette()[1])\n",
    "        plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "        ax2.set_ylabel('acquisition function', color=sns.color_palette()[1])\n",
    "        ax2.plot(X_design[i, :] * np.ones(100), np.linspace(0, af_values[i], 100), color=sns.color_palette()[1],\n",
    "                 linewidth=1)\n",
    "    return af_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we can use the code with the probability of improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the initial statistical model\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "gpr.likelihood.variance.constrain_fixed(1e-16)\n",
    "\n",
    "# Run the algorithm\n",
    "af_all = maximize(f, gpr, x[:, None], alpha=ei, max_it=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Rerun the main algorithm for EI by optimizing the hyper-parameters. Hint: Go through the code of ``maximize``."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
