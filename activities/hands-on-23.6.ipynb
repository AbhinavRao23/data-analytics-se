{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 23.6: Quantifying Epistemic Uncertainty about the Solution of the Optimization problem\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Quantify the epistemic uncertainty in the solution of an optimization problem\n",
    "\n",
    "## Quantifying our Epistemic Uncertainty About the Maximum\n",
    "\n",
    "Let's start by recreating our working example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 4 * (1. - np.sin(6 * x + 8 * np.exp(6 * x - 7.))) \n",
    "\n",
    "np.random.seed(123456) # For reproducibility\n",
    "n_init = 3\n",
    "X = np.random.rand(n_init) # In 1D you don't have to use LHS\n",
    "Y = f(X)\n",
    "plt.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, f(x), linewidth=2)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the previous hands-on activity, assume that we have made some observations and that we have used them to do Gaussian process regression resulting in the point-predictive distribution:\n",
    "$$\n",
    "p(y|\\mathbf{x},\\mathcal{D}_{n}) = \\mathcal{N}\\left(y|m_{n}(\\mathbf{x}), \\sigma^2_{n}(\\mathbf{x})\\right),\n",
    "$$\n",
    "where $m_{n}(\\mathbf{x})$ and $\\sigma^2_{n}(\\mathbf{x})$ are the predictive mean and variance respectively.\n",
    "Here is the code for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "# The kernel we use\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "# Assuming that we know there is no measurement noise:\n",
    "gpr.likelihood.variance.constrain_fixed(1e-16)\n",
    "# You can evaluate the predictive distribution anywhere:\n",
    "m, sigma2 = gpr.predict(x[:, None])\n",
    "# And you can visualize the results as follows\n",
    "# Standard deviation\n",
    "sigma = np.sqrt(sigma2)\n",
    "# Lower quantile\n",
    "l = m - 1.96 * sigma\n",
    "u = m + 1.96 * sigma\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "plt.plot(x, f(x), 'r--', linewidth=2, label='True function')\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='Observations')\n",
    "ax.plot(x, m, label='GP mean')\n",
    "ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25,\n",
    "                label='GP 95% pred. int.')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you have observed data $\\mathcal{D}_n$.\n",
    "How certain are you about the location of the maximum?\n",
    "If $n$ is small, you can't be too confident?\n",
    "How do you quantify this epistemic uncertainty?\n",
    "Notice that the maximum and the location of the maximum are *operators* acting on $f$:\n",
    "$$\n",
    "f^* := \\max[f] := \\max_{\\mathbf{x}} f(\\mathbf{x}),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathbf{x}^* := \\mathbf{X}^*[f] := \\arg\\max_{\\mathbf{x}} f(\\mathbf{x}),\n",
    "$$\n",
    "respectively.\n",
    "So since, we are uncertain about $f$, we will be uncertain about $f^*$ and $\\mathbf{x}^*$.\n",
    "In particular, we would like to somehow quantify the joint probability density $p(\\mathbf{x}^*, f^*|\\mathcal{D}_n)$.\n",
    "Here is what is the formal answer:\n",
    "$$\n",
    "p(\\mathbf{x}^*, f^*|\\mathcal{D}_n) = \\int \\delta(\\mathbf{x}^* - \\mathbf{X}^*[f])\\delta(f^*-\\max[f])p(f(\\cdot)|\\mathcal{D}_n)df(\\cdot).\n",
    "$$\n",
    "Of course, this is not technically correct because you cannot integrate over a function in this way.\n",
    "The correct way to write this mathematically is to use conditional expectations:\n",
    "$$\n",
    "p(\\mathbf{x}^*, f^*|\\mathcal{D}_n) = \\mathbb{E}\\left[\\delta(\\mathbf{x}^* - \\mathbf{X}^*[f])\\delta(f^*-\\max[f])|\\mathcal{D}_n\\right],\n",
    "$$\n",
    "where the expectation is taken over $f(\\cdot)$ conditional on $\\mathcal{D}_n$.\n",
    "In any case, there are two questions:\n",
    "+ What does this actually mean?\n",
    "+ How do you compute it?\n",
    "\n",
    "First, what does it actually mean?\n",
    "To understand this you need to pay attention to the delta function.\n",
    "Take for example $\\delta(f^* - \\max[f])$. What does it do?\n",
    "Well, it just hits a counter whenever $\\max[f]$ matches $f^*$ exactly as you take the expectation over $f(\\cdot)$.\n",
    "\n",
    "Second, how do you compute it?\n",
    "The simplest way to do this is through sampling.\n",
    "You basically just sample functions from $p(f(\\cdot)|\\mathcal{D}_n)$ and you find their maximum the location of the maximum.\n",
    "Of course, you cannot really sample a function.\n",
    "You sample the *function values* at a finite, but dense, number of input points and you find the maximum amongst these points.\n",
    "Once you get these samples, you just look at their histogram.\n",
    "\n",
    "Ok, let's do it for our working example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_max_and_argmax(gpr, X_design, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Plots histograms of the max and argmax of the function represented by the model gpr.\n",
    "    It takes ``n_samples`` samples from the posterior to do that evaluated at ``X_design``.\n",
    "    \"\"\"\n",
    "    f_samples = gpr.posterior_samples_f(X_design, size=n_samples)[:, 0, :]\n",
    "    max_f_samples = np.max(f_samples, axis=0)\n",
    "    x_star_samples = X_design[np.argmax(f_samples, axis=0), 0]\n",
    "    fig, ax = plt.subplots(dpi=100)\n",
    "    ax.hist(max_f_samples, density=True, alpha=0.25)\n",
    "    ax.set_xlabel('$f^*$')\n",
    "    ax.set_ylabel('$p(f^*|\\mathcal{D}_n)$')\n",
    "    fig, ax = plt.subplots(dpi=100)\n",
    "    ax.hist(x_star_samples, density=True, alpha=0.25)\n",
    "    ax.set_xlabel('$x^*$')\n",
    "    ax.set_ylabel('$p(x^*|\\mathcal{D}_n)$')\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_and_argmax(gpr, x[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this functionality to our maximization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mui(m, sigma, ymax, psi=1.96):\n",
    "    \"\"\"\n",
    "    Return the maximum upper interval.\n",
    "    \"\"\"\n",
    "    return m + psi * sigma\n",
    "\n",
    "def poi(m, sigma, ymax, psi=0.):\n",
    "    \"\"\"\n",
    "    Return the probability of improvement.\n",
    "    \"\"\"\n",
    "    return st.norm.cdf((m - ymax - psi) / sigma)\n",
    "\n",
    "def ei(m, sigma, ymax, psi=0.):\n",
    "    \"\"\"\n",
    "    Return the expected improvement.\n",
    "    \"\"\"\n",
    "    u = (m - ymax) / sigma\n",
    "    ei = sigma * (u * st.norm.cdf(u) + st.norm.pdf(u))\n",
    "    ei[sigma <= 0.] = 0.\n",
    "    return ei\n",
    "\n",
    "def maximize(f, gpr, X_design, alpha, psi=0., max_it=6, plot_epistemic=True):\n",
    "    \"\"\"\n",
    "    Optimize f using a limited number of evaluations.\n",
    "    \n",
    "    :param f:        The function to optimize.\n",
    "    :param gpr:      A Gaussian process model to use for representing our state of knowldege.\n",
    "    :param X_design: The set of candidate points for identifying the maximum.\n",
    "    :param alpha:    The acquisition function.\n",
    "    :param psi:      The parameter value for the acquisition function (not used for EI).\n",
    "    :param max_it:   The maximum number of iterations.\n",
    "    \"\"\"\n",
    "    af_all = []\n",
    "    for count in range(max_it):\n",
    "        m, sigma2 = gpr.predict(X_design)\n",
    "        sigma = np.sqrt(sigma2)\n",
    "        l = m - 1.96 * sigma\n",
    "        u = m + 1.96 * sigma\n",
    "        af_values = alpha(m, sigma, gpr.Y.max(), psi=psi)\n",
    "        i = np.argmax(af_values)\n",
    "        X = np.vstack([gpr.X, X_design[i:(i+1), :]])\n",
    "        y = np.vstack([gpr.Y, [f(X_design[i, :])]])\n",
    "        gpr.set_XY(X, y)\n",
    "        # Uncomment the following to optimize the hyper-parameters\n",
    "        # gpr.optimize()\n",
    "        af_all.append(af_values[i])\n",
    "        fig, ax = plt.subplots(dpi=100)\n",
    "        ax.set_title('BGO Iteration #{0:d}'.format(count+1))\n",
    "        ax.plot(gpr.X, gpr.Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$y$')\n",
    "        ax.plot(x, m)\n",
    "        ax.fill_between(X_design.flatten(), l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25)\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(X_design, af_values, color=sns.color_palette()[1])\n",
    "        plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "        ax2.set_ylabel('acquisition function', color=sns.color_palette()[1])\n",
    "        ax2.plot(X_design[i, :] * np.ones(100), np.linspace(0, af_values[i], 100), color=sns.color_palette()[1],\n",
    "                 linewidth=1)\n",
    "        if plot_epistemic:\n",
    "            fig, ax = plot_max_and_argmax(gpr, X_design)\n",
    "            ax.set_title('BGO Iteration #{0:d}'.format(count+1))\n",
    "    return af_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a complete run of the lagorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the initial statistical model\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "gpr.likelihood.variance.constrain_fixed(1e-16)\n",
    "\n",
    "# Run the algorithm\n",
    "af_all = maximize(f, gpr, x[:, None], alpha=ei, psi=0., max_it=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ How does the epistemic uncertainty about the optimization problem change when you decrease the number of initial samples?\n",
    "\n",
    "+ Try changing the number of initial samples to a very small number? Does the algorithm work?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
