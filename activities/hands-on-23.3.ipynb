{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 23.3: Probability of Improvement\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Develop intuition about the probability of improvement\n",
    "\n",
    "## Probability of Improvement\n",
    "Let's reintroduce the same running example as the previous hands-on activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 4 * (1. - np.sin(6 * x + 8 * np.exp(6 * x - 7.))) \n",
    "\n",
    "np.random.seed(123456) # For reproducibility\n",
    "n_init = 3\n",
    "X = np.random.rand(n_init) # In 1D you don't have to use LHS\n",
    "Y = f(X)\n",
    "plt.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "x = np.linspace(0, 1)\n",
    "plt.plot(x, f(x), linewidth=2)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the previous hands-on activity, assume that we have made some observations and that we have used them to do Gaussian process regression resulting in the point-predictive distribution:\n",
    "$$\n",
    "p(y|\\mathbf{x},\\mathcal{D}_{n}) = \\mathcal{N}\\left(y|m_{n}(\\mathbf{x}), \\sigma^2_{n}(\\mathbf{x})\\right),\n",
    "$$\n",
    "where $m_{n}(\\mathbf{x})$ and $\\sigma^2_{n}(\\mathbf{x})$ are the predictive mean and variance respectively.\n",
    "Here is the code for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "# The kernel we use\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "# Assuming that we know there is no measurement noise:\n",
    "gpr.likelihood.variance.constrain_fixed(1e-16)\n",
    "# You can evaluate the predictive distribution anywhere:\n",
    "m, sigma2 = gpr.predict(x[:, None])\n",
    "# And you can visualize the results as follows\n",
    "# Standard deviation\n",
    "sigma = np.sqrt(sigma2)\n",
    "# Lower quantile\n",
    "l = m - 1.96 * sigma\n",
    "u = m + 1.96 * sigma\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "plt.plot(x, f(x), 'r--', linewidth=2, label='True function')\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='Observations')\n",
    "ax.plot(x, m, label='GP mean')\n",
    "ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25,\n",
    "                label='GP 95% pred. int.')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you try to find the point that maximizes the probability of getting an observation greater than the ones you have so far?\n",
    "Let's derive this.\n",
    "First, let's call $y_n^*$ the current maximum in your dataset, i.e.,\n",
    "$$\n",
    "y_n^* = \\max_{1\\le i\\le n}y_i.\n",
    "$$\n",
    "We define the following acquisition function:\n",
    "$$\n",
    "a_n(\\mathbf{x}) = \\mathbb{P}[y > y_n^* + \\psi | \\mathbf{x}, \\mathcal{D}_n].\n",
    "$$\n",
    "We read \"$a_n(\\mathbf{x})$\" is the probability that we observe at $x$ a $y$ that is greater than the currently observed maximum $y_n^*$ by at least $\\psi>0$.\n",
    "The good thing is that it is possible to get an analytical answer because our point predictive distribution is Gaussian.\n",
    "In particular, we get:\n",
    "$$\n",
    "\\begin{align}\n",
    "a_n(\\mathbf{x}) &=& \\mathbb{P}[y > y_n^* + \\psi | x, \\mathcal{D}_n]\\\\\n",
    "&=& \\mathbb{P}\\left[\\frac{y - \\mu_n(\\mathbf{x})}{\\sigma_n(\\mathbf{x})} > \\frac{y_n^* + \\psi - \\mu_n(\\mathbf{x})}{\\sigma_n(\\mathbf{x})} \\Big| \\mathbf{x}, \\mathcal{D}_n\\right]\\\\\n",
    "&=& 1 - \\mathbb{P}\\left[\\frac{y - \\mu_n(\\mathbf{x})}{\\sigma_n(\\mathbf{x})} \\le \\frac{y_n^* + \\psi - \\mu_n(\\mathbf{x})}{\\sigma_n(\\mathbf{x})} \\Big| \\mathbf{x}, \\mathcal{D}_n\\right]\\\\\n",
    "&=& 1 - \\Phi\\left(\\frac{y_n^* + \\psi - \\mu_n(\\mathbf{x})}{\\sigma_n(\\mathbf{x})} \\right)\\\\\n",
    "&=& \\Phi\\left(\\frac{\\mu_n(\\mathbf{x}) - y_n^* - \\psi}{\\sigma_n(\\mathbf{x})} \\right),\n",
    "\\end{align}\n",
    "$$\n",
    "where we used that since $y_n | \\mathbf{x}_n, \\mathcal{D}_n$ is Gaussian with mean $\\mu_n(\\mathbf{x})$ and variance $\\sigma_n^2(\\mathbf{x})$, then $\\frac{y_n-\\mu_n(\\mathbf{x})}{\\sigma_n(\\mathbf{x})}$ is a standard normal, and we also used that the CDF of the standard normal satisfies this property:\n",
    "$$\n",
    "\\Phi(-z) = 1 - \\Phi(z).\n",
    "$$\n",
    "\n",
    "Here is the code for this activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poi(m, sigma, ymax, psi=0.):\n",
    "    \"\"\"\n",
    "    Return the probability of improvement.\n",
    "    \n",
    "    Arguments:\n",
    "    m        -      the predictive mean at the test points.\n",
    "    sigma        -  the predictive standard deviation at the test points.\n",
    "    ymax     -      the maximum observed value (so far).\n",
    "    psi      -      a parameter that controls exploration.\n",
    "    \"\"\"\n",
    "    return st.norm.cdf((m - ymax - psi) / sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_poi(psi=0.):\n",
    "    fig, ax = plt.subplots(dpi=100)\n",
    "    ax.set_title('$\\psi={0:1.2f}$'.format(psi))\n",
    "    ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.plot(x, m)\n",
    "    ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25)\n",
    "    af_values = poi(m, sigma, Y.max(), psi)\n",
    "    next_id = np.argmax(af_values)\n",
    "    next_x = x[next_id]\n",
    "    af_max = af_values[next_id]\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x, af_values, color=sns.color_palette()[1])\n",
    "    ax2.set_ylabel('Probability of Improvement', color=sns.color_palette()[1])\n",
    "    plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "    ax2.plot(next_x * np.ones(100), np.linspace(0, af_max, 100), color=sns.color_palette()[1],\n",
    "         linewidth=1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "from ipywidgets import interactive, interact_manual\n",
    "    \n",
    "interactive(plot_poi, psi=(0., 4., 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ Experiment with different values of $\\psi$.\n",
    "+ When do you get exploration?\n",
    "+ When do you get exploitation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian global optimization with the probability of improvement\n",
    "\n",
    "Let's now run the Bayesian global optimization algorithm using the probability of improvement as the information acquisition function.\n",
    "Here is the generic code from the previous hands-on activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize(f, gpr, X_design, alpha, psi=0., max_it=6):\n",
    "    \"\"\"\n",
    "    Optimize f using a limited number of evaluations.\n",
    "    \n",
    "    :param f:        The function to optimize.\n",
    "    :param gpr:      A Gaussian process model to use for representing our state of knowldege.\n",
    "    :param X_design: The set of candidate points for identifying the maximum.\n",
    "    :param alpha:    The acquisition function.\n",
    "    :param psi:      The parameter value for the acquisition function (not used for EI).\n",
    "    :param max_it:   The maximum number of iterations.\n",
    "    \"\"\"\n",
    "    af_all = []\n",
    "    for count in range(max_it):\n",
    "        m, sigma2 = gpr.predict(X_design)\n",
    "        sigma = np.sqrt(sigma2)\n",
    "        l = m - 1.96 * sigma\n",
    "        u = m + 1.96 * sigma\n",
    "        af_values = alpha(m, sigma, gpr.Y.max(), psi=psi)\n",
    "        i = np.argmax(af_values)\n",
    "        X = np.vstack([gpr.X, X_design[i:(i+1), :]])\n",
    "        y = np.vstack([gpr.Y, [f(X_design[i, :])]])\n",
    "        gpr.set_XY(X, y)\n",
    "        # Uncomment the following to optimize the hyper-parameters\n",
    "        # gpr.optimize()\n",
    "        af_all.append(af_values[i])\n",
    "        fig, ax = plt.subplots(dpi=100)\n",
    "        ax.plot(gpr.X, gpr.Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$y$')\n",
    "        ax.plot(x, m)\n",
    "        ax.fill_between(X_design.flatten(), l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25)\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(X_design, af_values, color=sns.color_palette()[1])\n",
    "        plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "        ax2.set_ylabel('acquisition function', color=sns.color_palette()[1])\n",
    "        ax2.plot(X_design[i, :] * np.ones(100), np.linspace(0, af_values[i], 100), color=sns.color_palette()[1],\n",
    "                 linewidth=1)\n",
    "    return af_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we can use the code with the probability of improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the initial statistical model\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "gpr.likelihood.variance.constrain_fixed(1e-16)\n",
    "\n",
    "# Run the algorithm\n",
    "af_all = maximize(f, gpr, x[:, None], alpha=poi, psi=0., max_it=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Repeat the main algorithm using POI for a $\\psi$ that exploits. Does the method converge?\n",
    "+ Repeat the main algorithm using POI for a $\\psi$ that explores. Does the method converge?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
