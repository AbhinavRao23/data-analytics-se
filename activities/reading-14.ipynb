{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Activity 14 - Bayesian Linear Regression\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To introduce the probabilistic interpretation of least squares\n",
    "\n",
    "## Probabilistic interpretation of least squares (maximum likelihood)\n",
    "\n",
    "We wish to model the data using some **fixed** basis/features:\n",
    "$$\n",
    "y(\\mathbf{x};\\mathbf{w}) = \\sum_{j=1}^{m} w_{j}\\phi_{j}(\\mathbf{x}) = \\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})\n",
    "$$\n",
    "However, instead of directly picking a loss function to minimize we come up with a probabilistic description of the measurement process.\n",
    "In particular, we *model the measurement process* using a **likelihood** function:\n",
    "$$\n",
    "\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w} \\sim p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}).\n",
    "$$\n",
    "\n",
    "What is the interpretation of the likelihood function?\n",
    "Well, $p(\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w})$ tells us how plausible is it to observe $\\mathbf{y}_{1:n}$ at inputs $\\mathbf{x}_{1:n}$, if we know that the model parameters are $\\mathbf{w}$.\n",
    "\n",
    "Since, in almost all the cases we consider, the measurements are independent conditioned on the model, then likelihood of the data factorizes as follows:\n",
    "$$\n",
    "p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n}, \\mathbf{w}) = \\prod_{i=1}^np(y_i|\\mathbf{x}_i, \\mathbf{w}),\n",
    "$$\n",
    "where $p(y_i|\\mathbf{x}_i,\\mathbf{w})$ is the likelihood of a single measurement.\n",
    "\n",
    "The most common choice for the likehood of a single measurement is to pick it to be Gaussian.\n",
    "We assign:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "p(y_i|\\mathbf{x}_i, \\mathbf{w}, \\sigma) &=& \\mathcal{N}\\left(y_i| y(\\mathbf{x}_i;\\mathbf{w}), \\sigma^2\\right)\\\\\n",
    "&=& \\mathcal{N}\\left(y_i | \\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)}, \\sigma^2\\right),\n",
    "\\end{array}\n",
    "$$\n",
    "where $\\sigma$ models the **noise**.\n",
    "This correspond to the belief that our measurement is around the model prediction $\\mathbf{w^{T}\\boldsymbol{\\phi}(\\mathbf{x})}$\n",
    "but it is contaminated with Gaussian noice of variance $\\sigma^2$.\n",
    "\n",
    "Assuming a Gaussian likelihood for a single observation, we have for all the data:\n",
    "$$\n",
    "p(\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma) = \\mathcal{N}\\left(\\mathbf{y}_{1:n} | \\mathbf{\\Phi}\\mathbf{w}, \\sigma^2\\mathbf{I}_n\\right).\n",
    "$$\n",
    "Let's look up the form of the multivariate Gaussian from the ([Wiki](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)):\n",
    "$$\n",
    "p(\\mathbf{y}_{1:n} | \\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma) \n",
    "= (2\\pi)^{-\\frac{n}{2}}\\sigma^{-n} e^{-\\frac{1}{2\\sigma^2}\\lVert\\mathbf{\\Phi}\\mathbf{w}-\\mathbf{y}_{1:n}\\rVert^2}.\n",
    "$$\n",
    "\n",
    "### Maximum Likelihood Estimate of $\\mathbf{w}$\n",
    "\n",
    "Once we have a likelihood, we can train the model by maximizing the likelihood:\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{MLE}} = \\arg\\max_{\\mathbf{w}} p(\\mathbf{y}_{1:n}, |\\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma).\n",
    "$$\n",
    "When we do this we are essentially selecting the model that makes the observations most likely.\n",
    "For the Gaussian likelihood, we have:\n",
    "$$\n",
    "\\log p(\\mathbf{y}_{1:n}, |\\mathbf{x}_{1:n}, \\mathbf{w}, \\sigma) =\n",
    "-\\frac{n}{2}\\log(2\\pi)\n",
    "-n\\log\\sigma\n",
    "- \\frac{1}{2\\sigma^2}\\lVert\\mathbf{\\Phi}\\mathbf{w}-\\mathbf{y}_{1:n}\\rVert^2.\n",
    "$$\n",
    "Taking the derivatives of this expression with respect to $\\mathbf{w}$ and setting them equal to zero (sufficient condition) yields the same solution as least squares.\n",
    "$$\n",
    "\\mathbf{w}_{\\mbox{MLE}} \\equiv \\mathbf{w}_{\\mbox{LS}}.\n",
    "$$\n",
    "\n",
    "### Maximum Likelihood Estimate of $\\sigma$\n",
    "The probabilistic interpretation above gives the same solution as least squares.\n",
    "To start undersanding its power, notice that it can also give us an estimate for the measurement noise variance $\\sigma^2$.\n",
    "All you have to do is maximize likelihood with respect to $\\sigma$.\n",
    "For the Gaussian likelihood:\n",
    "\n",
    "+ Take the derivative of $p(\\mathbf{y}_{1:n}|\\mathbf{x}_{1:n},\\mathbf{w}_{\\mbox{MLE}},\\sigma)$ with respect to $\\sigma$.\n",
    "+ Set to zero, and solve for $\\sigma$.\n",
    "+ You will get:\n",
    "$$\n",
    "\\sigma_{\\mbox{MLE}}^2 = \\frac{\\lVert \\mathbf{\\Phi}\\mathbf{w} - \\mathbf{y}_{1:n}\\rVert^2}{n}.\n",
    "$$\n",
    "\n",
    "### Making Predictions\n",
    "How do we make predictions about $y$ at a new point $\\mathbf{x}$?\n",
    "We just use the laws of probability...\n",
    "For the Gaussian likelihood, the **point predictive distribution** is:\n",
    "$$\n",
    "p(y|\\mathbf{x}, \\mathbf{w}_{\\mbox{MLE}}, \\sigma^2_{\\mathbf{\\mbox{MLE}}}) = \n",
    "\\mathcal{N}\\left(y\\middle|\\mathbf{w}_{\\mbox{MLE}}^T\\mathbf{\\phi}(\\mathbf{x}), \\sigma_{\\mbox{MLE}}^2\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
