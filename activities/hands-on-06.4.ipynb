{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Activity 6.3 The Multivariate Normal - Conditioning\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To demonstrate conditioning of a multivariate normal.\n",
    "\n",
    "## The multivariate mormal - Conditioning\n",
    "\n",
    "Consider the $N$-dimensional multivariate normal:\n",
    "$$\n",
    "\\mathbf{X} \\sim N\\left(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right),\n",
    "$$\n",
    "where $\\boldsymbol{\\mu}$ is a $N$-dimensional vector, $\\boldsymbol{\\Sigma}$ is a *positive-definite matrix*.\n",
    "Assume that $\\boldsymbol{\\mu}$ can be decomposed in two blocks of dimensions $N_1$ and $N_2$ ($N_1 + N_2 = N$):\n",
    "$$\n",
    "\\boldsymbol{\\mu} =\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{\\mu}_1\\\\\n",
    "\\boldsymbol{\\mu}_2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "Similarly for $\\boldsymbol{\\Sigma}$:\n",
    "$$\n",
    "\\boldsymbol{\\Sigma} = \\begin{pmatrix}\n",
    "\\boldsymbol{\\Sigma}_1 & \\boldsymbol{\\Sigma}_{12}\\\\\n",
    "\\boldsymbol{\\Sigma}_{12}^T&\\boldsymbol{\\Sigma}_2\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "where $\\boldsymbol{\\Sigma}_{ii}$ are $N_i\\times N_i$ matrices, and $\\boldsymbol{\\Sigma}_{12}$ is a $N_1\\times N_2$ matrix.\n",
    "In lecture, we saw that when you observe that $\\mathbf{X}_2 = \\mathbf{x}_2$, then $\\mathbf{X}_1$ is distributed according to:\n",
    "$$\n",
    "\\mathbf{X}_1|\\mathbf{X}_2 = \\mathbf{x}_2\\sim N(\\boldsymbol{\\mu}_1+\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_2^{-1}(\\mathbf{x}_2-\\boldsymbol{\\mu}_2), \\boldsymbol{\\Sigma}_1-\\boldsymbol{\\Sigma}_{12}\\boldsymbol{\\Sigma}_2^{-1}\\boldsymbol{\\Sigma}_{12}^T).\n",
    "$$\n",
    "Let's demonstrate this first for the case $N=2$ and $N_1=N_2=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "# The mean vector\n",
    "mu = [1.0, 2.0]\n",
    "# The covariance matrix\n",
    "Sigma = np.array([[1.0, 0.9],\n",
    "                   [0.9, 1.0]])\n",
    "# The multivariate normal random vector\n",
    "X = st.multivariate_normal(mean=mu, cov=Sigma)\n",
    "\n",
    "# Let's say that we observe the x2 component of a sample of X\n",
    "x_sample = X.rvs()\n",
    "# The x2 component is observed\n",
    "x2_observed = x_sample[1]\n",
    "# The x1 component is hidden\n",
    "x1_hidden = x_sample[0]\n",
    "\n",
    "# Let's plot the contour of the joint and see where $x2$ falls\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "# The contours\n",
    "# Points along x1 dimension\n",
    "x1 = np.linspace(-3, 5, 64)\n",
    "# Points along x2 dimension\n",
    "x2 = np.linspace(-3, 5, 64)\n",
    "# Create grid\n",
    "Xg1, Xg2 = np.meshgrid(x1, x2)\n",
    "# Flattened values of grid points\n",
    "Xg_flat = np.hstack([Xg1.flatten()[:, None], Xg2.flatten()[:, None]])\n",
    "# PDF values\n",
    "pdf_Xg = X.pdf(Xg_flat).reshape(Xg1.shape)\n",
    "# Plot contours\n",
    "c = ax.contour(Xg1, Xg2, pdf_Xg)\n",
    "plt.colorbar(c, label='$p(\\mathbf{x})$')\n",
    "\n",
    "# SAMPLES\n",
    "num_samples = 500\n",
    "# Plot the mean\n",
    "ax.plot(X.mean[0], X.mean[1], 'ro', label='$\\mu$')\n",
    "# Plot the observed x2 as a line\n",
    "ax.plot(x1, [x2_observed] * np.ones(x1.shape[0]), '--', label='Observed $x_2$')\n",
    "plt.legend(loc='best')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, intuitively, conditioned on known $X_2 = x_2$ the values of $X_1$ must have higher probability in the intersection of the dashed line with the contours of the joint.\n",
    "Let's see what is the answer we get from the theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to extract some info from the covariance matrix\n",
    "Sigma11 = Sigma[0, 0]\n",
    "Sigma12 = Sigma[0, 1]\n",
    "Sigma22 = Sigma[1,1]\n",
    "# In the general case (N1, N2 != 1), these would have been matrices.\n",
    "# Here it is a bit easier because they are all scalars.\n",
    "# Let's do the same for the mean vector\n",
    "mu1 = mu[0]\n",
    "mu2 = mu[1]\n",
    "# Let's find the conditioned mean:\n",
    "mu1_cond = mu1 + Sigma12 * (x2_observed - mu2) / Sigma22\n",
    "# Let's find the conditioed variance:\n",
    "Sigma11_cond = Sigma11 - Sigma12 ** 2 / Sigma22\n",
    "# Let's plot the pdf of this along with the hidden (unobserved) value of x1 which is known in this toy example:\n",
    "X1_cond = st.norm(loc=mu1_cond, scale=np.sqrt(Sigma11_cond))\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x1, X1_cond.pdf(x1), label='$p(x_1|x_2)$')\n",
    "ax.plot([x1_hidden], [0], 'rd', label='True hidden value of $x_1$')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('Conditional probability')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "+ Rerun the code above multiple times to see how the conditinal PDF moves around as other points are picked randomly.\n",
    "+ Modify the code so that you get the conditional PDF of $X_2$ given $X_1=x_1$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
