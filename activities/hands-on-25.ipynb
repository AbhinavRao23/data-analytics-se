{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 25 - Deep Neural Networks Continued\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Implement image classification network in `PyTorch`.\n",
    "+ Add L2 regularization.\n",
    "+ Add convolutional layers.\n",
    "+ Add hyperparameter tuning.\n",
    "\n",
    "## References \n",
    "\n",
    "+ [Deep Learning with PyTorch: A 60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) and in particular:\n",
    "    - [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) - with which we use the same dataset in this hands-on activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CIFAR10 dataset\n",
    "\n",
    "We are going to use the [CIFAR10 dateset](https://www.cs.toronto.edu/~kriz/cifar.html) to demonstrate multiclass classification.\n",
    "The dataset consists of 60000 32x32 color images in 10 classes (plane, car, bird, cat, deer, dog, frog, horse, ship, and truck), with 6000 images per class.\n",
    "The dataset can be download direclty from `PyTorch` using the module `torchvision`.\n",
    "\n",
    "You can think of the original images as 32x32x3 arrays.\n",
    "The first two dimensions correspond to the pixels.\n",
    "The third dimension corresponds to the color (red, green, blue).\n",
    "Of course, we will have to turn them into `PyTorch` tensors.\n",
    "Also, it is more convenient to scale them to be between $[-1,1]$.\n",
    "We will achieve this using a transformation.\n",
    "Don't worry about this now.\n",
    "We will explain it as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# This is the transformation that we will apply to each image\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),   # This turns the picture to a Tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # This scales it to [-1, 1]\n",
    "\n",
    "# Here is how you can download the training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# And here is how to download the test dataset:\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# These are the class labels\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all these data went in the folder \"./data.\"\n",
    "Here is what this folder contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mcifar-10-batches-py\u001b[m\u001b[m    cifar-10-python.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `cifar-10-python.tar.gz` is a compressed file containing everything.\n",
    "The contents were automatically extracted and put in the folder `cifar-10-batches-py`.\n",
    "Let's look insider this folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 363752\r\n",
      "-rw-r--r--@ 1 iliasbilionis  staff    88B Jun  4  2009 readme.html\r\n",
      "-rw-r--r--@ 1 iliasbilionis  staff   158B Mar 31  2009 batches.meta\r\n",
      "-rw-r--r--@ 1 iliasbilionis  staff    30M Mar 31  2009 data_batch_4\r\n",
      "-rw-r--r--@ 1 iliasbilionis  staff    30M Mar 31  2009 data_batch_1\r\n",
      "-rw-r--r--@ 1 iliasbilionis  staff    30M Mar 31  2009 data_batch_5\r\n",
      "-rw-r--r--@ 1 iliasbilionis  staff    30M Mar 31  2009 data_batch_2\r\n",
      "-rw-r--r--@ 1 iliasbilionis  staff    30M Mar 31  2009 data_batch_3\r\n",
      "-rw-r--r--@ 1 iliasbilionis  staff    30M Mar 31  2009 test_batch\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lht data/cifar-10-batches-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see several files.\n",
    "The important ones are `data_batch_1` to `data_batch_5` and `test_batch`.\n",
    "Each of these contains 10000 images in a binary format.\n",
    "The format is explained [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "We can read them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "data = unpickle('data/cifar-10-batches-py/data_batch_1')\n",
    "# data is a dictionary\n",
    "# Here are the keys\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# One key has to do with the pictures\n",
    "# It gives you a numpy array:\n",
    "print(data[b'data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first dimension correspond to differnt picture\n",
    "# The second dimension is\n",
    "32 * 32 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 59  16  25 ... 208 180 177]\n",
      " [ 43   0  16 ... 201 173 168]\n",
      " [ 50  18  49 ... 198 186 179]\n",
      " ...\n",
      " [158 123 118 ... 160 184 216]\n",
      " [152 119 120 ...  56  97 151]\n",
      " [148 122 109 ...  53  83 123]]\n"
     ]
    }
   ],
   "source": [
    "# So this is the first picture:\n",
    "img = data[b'data'][0, :].reshape((32, 32, 3), order='F')\n",
    "# Here is the Red channel:\n",
    "print(img[:, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers go from 0 (no red) to 255 (full red).\n",
    "Here is how to visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(1, 1))\n",
    "ax.imshow(np.transpose(img, (1, 0, 2)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly a frog.\n",
    "Let's verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[data[b'labels'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nice. And we could proceed manually like this.\n",
    "However, `PyTorch` offers some useful functionality.\n",
    "Let's investigate the `trainset` that was returned by `CIFAR10`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are the classes:\n",
    "trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'automobile': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'deer': 4,\n",
       " 'dog': 5,\n",
       " 'frog': 6,\n",
       " 'horse': 7,\n",
       " 'ship': 8,\n",
       " 'truck': 9}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the correspondence between classes and discrete labels\n",
    "trainset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Here are the images from all training batches\n",
    "print(trainset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "# Here are the labels\n",
    "print(trainset.targets[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright.\n",
    "Now, let's use `PyTorch` functionality for looping over the training and the test datasets.\n",
    "We need a [DataLoader](https://pytorch.org/docs/stable/data.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One for the training data:\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# One for the test data:\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects work as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data point: 0 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 1000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 2000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 3000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 4000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 5000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 6000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 7000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 8000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 9000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 10000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 11000 input size: torch.Size([4, 3, 32, 32])\n",
      "Data point: 12000 input size: torch.Size([4, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# They help you loop over all the data in a random way (because we had shuffle=True)\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data\n",
    "    # Here inputs are of size batch_size x (3 x 32 x 32)\n",
    "    # Since we had specified, the batch_size to be 4\n",
    "    # this essentially loads four images per iteration\n",
    "    if i % 1000 == 0:\n",
    "        print('Data point:', i, 'input size:', str(inputs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you reach the end of the loop you have visited all the images once.\n",
    "Notice that `PyTorch` has reshaped the images to 3 x 32 x 32 3D arrays.\n",
    "This is more convenient for the convolutional layers we are going to use later.\n",
    "Also, `PyTorch` is using the transformations we gave it to scale the data to array elements to $[-1, 1]$.\n",
    "Let me show you an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1608,  0.0196, -0.0431,  ..., -0.9373, -0.8431, -0.3961],\n",
      "         [-0.1294,  0.0196, -0.1686,  ..., -0.9529, -0.4902, -0.1451],\n",
      "         [-0.4510, -0.3176, -0.2627,  ..., -0.8667, -0.5137, -0.3255],\n",
      "         ...,\n",
      "         [ 0.4980,  0.3725,  0.4510,  ...,  0.3098,  0.3098,  0.3255],\n",
      "         [ 0.4588,  0.3882,  0.4353,  ...,  0.3412,  0.3490,  0.3647],\n",
      "         [ 0.4196,  0.4353,  0.4902,  ...,  0.4353,  0.4431,  0.4902]],\n",
      "\n",
      "        [[-0.1294, -0.2863, -0.3804,  ..., -0.9843, -0.9373, -0.5922],\n",
      "         [-0.3804, -0.2706, -0.5216,  ..., -0.9843, -0.6235, -0.4431],\n",
      "         [-0.6627, -0.5765, -0.5765,  ..., -0.9451, -0.6941, -0.6392],\n",
      "         ...,\n",
      "         [ 0.1216,  0.0275,  0.1216,  ..., -0.0667, -0.0745, -0.0588],\n",
      "         [ 0.0824,  0.0196,  0.0667,  ..., -0.0118, -0.0196, -0.0039],\n",
      "         [ 0.0588,  0.0745,  0.1137,  ...,  0.0980,  0.0824,  0.1216]],\n",
      "\n",
      "        [[-0.2471, -0.4510, -0.5216,  ..., -0.9059, -0.9216, -0.6627],\n",
      "         [-0.4902, -0.4118, -0.6078,  ..., -0.9529, -0.6314, -0.4745],\n",
      "         [-0.7647, -0.6863, -0.6157,  ..., -0.9686, -0.7255, -0.6392],\n",
      "         ...,\n",
      "         [-0.0118, -0.0980,  0.0039,  ..., -0.1451, -0.1451, -0.1529],\n",
      "         [-0.0667, -0.0980, -0.0275,  ..., -0.1843, -0.1765, -0.1451],\n",
      "         [-0.1373, -0.0902, -0.0196,  ..., -0.1059, -0.0980, -0.0275]]])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data\n",
    "    print(inputs[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a classifier using a dense DNNs\n",
    "\n",
    "Let's just train a classifer using a dense neural network.\n",
    "It's not going to work very well, but it is very easy to put together.\n",
    "We are going to start the network with 3 x 32 x 32 = 3072, followed up with a few dense layers that end at 10 outputs passed through softmax.\n",
    "However, for reasons of numerical stability, we are not going to end with the softmax layer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# The classifer - The dimensions of the layers have\n",
    "# been picked to match those of the convolutional neural network\n",
    "# that we are going to build later\n",
    "# For now, just notice that we gradually take the 3072-dimensional input\n",
    "# down to 10 dimensions (the number of classes we have)\n",
    "# Also, notice that I do not add the softmax layer at this point\n",
    "model_dense = nn.Sequential(nn.Linear(3072, 1176), nn.ReLU(),\n",
    "                            nn.Linear(1176, 400), nn.ReLU(),\n",
    "                            nn.Linear(400, 120), nn.ReLU(),\n",
    "                            nn.Linear(120, 84), nn.ReLU(),\n",
    "                            nn.Linear(84, 10))\n",
    "\n",
    "# This is our loss function. \n",
    "# Read this: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# The reason we did not add the Softmax layer at the end is because\n",
    "# the loss function above is doing it internally.\n",
    "# It expects that you provide \"contain raw, unnormalized scores for each class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model_dense.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network. This is going to take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-a1522d12a0d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# How many times do you want to go over the entire dataset?\n",
    "# Don't pick a very big number because you will overfit\n",
    "num_epochs = 10\n",
    "\n",
    "# Here is the main training algorithm\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_dense(inputs.reshape(4, 3 * 32 * 32))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training networks takes a while, it's a good idea to save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_dense.state_dict(), 'hands-on-25-model-dense.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is as a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lht hands-on-25-model-dense.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first four images and their labels\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the net and pass them through \n",
    "# softmax to turn them into probabilities\n",
    "st = nn.Softmax(dim=1)\n",
    "predictions = st(model_dense(inputs.reshape(4, 3072)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, ax):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# Plot the pictures and the predictions\n",
    "for i in range(4):\n",
    "    fig, ax = plt.subplots(figsize=(1,1))\n",
    "    imshow(images[i], ax)\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    ax2.bar(np.arange(10), predictions[i].detach().numpy())\n",
    "    ax2.set_xticks(np.arange(10))\n",
    "    ax2.set_xticklabels(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
