{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 23.2: Maximum Upper Interval\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Develop intuition about the maximum upper interval acquisition function\n",
    "\n",
    "## Exploration vs Exploitation\n",
    "\n",
    "The question is this: \"Where should we evaluate the function next if our goal is to maximize it?\"\n",
    "Two possibilities of choosing a point for the next evaluation are:\n",
    "+ **Exploitation:** We can choose a point $\\mathbf{x}$ that *exploits* our current state of knowledge by concentrating on the region where the model thinks the maximum is. In our working example, that would be the point right next to the left most observation.\n",
    "+ **Exploration:** We can *explore* the regions of maximum predictive uncertainty because there is a high chance that they may hide the maximum of the function. In our working example, this is the region between the two observations on the right.\n",
    "\n",
    "Generally speaking, it is a very bad idea to focus exclusive on either on exploration or exploitation.\n",
    "On one hand, if we focus on exploration, then we will at the end recover the true response surface (and as a consequence we will get the correct maximum of the function) but we waste a lot of evaluations on regions that are very unlikely to contain the maximum.\n",
    "If on the other hand we focus on exploitation, then we will very quickly converge to a local maximum, maybe a good one maybe a bad one, and a lot of the input space will remain unexplored, see the previous hands-on activity.\n",
    "\n",
    "So, what should a good *information acquisition function* $a_n(\\mathbf{x})$ for optimization do?\n",
    "It should *strike a balance between exploration and exploitation* in a way that provably reveals, in the limit of large number of evaluations, the global maximum of the function.\n",
    "Are there such information acquisition algorithms? Yes there are.\n",
    "We are going to explore the first such acquisition function, the maximum upper interval, in this hands-on activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reintroduce the same running example as the previous hands-on activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 4 * (1. - np.sin(6 * x + 8 * np.exp(6 * x - 7.))) \n",
    "\n",
    "np.random.seed(123456) # For reproducibility\n",
    "n_init = 3\n",
    "X = np.random.rand(n_init) # In 1D you don't have to use LHS\n",
    "Y = f(X)\n",
    "plt.plot(X, Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "x = np.linspace(0, 1)\n",
    "plt.plot(x, f(x), linewidth=2)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum upper interval\n",
    "\n",
    "Just like in the previous hands-on activity, assume that we have made some observations and that we have used them to do Gaussian process regression resulting in the point-predictive distribution:\n",
    "$$\n",
    "p(y|\\mathbf{x},\\mathcal{D}_{n}) = \\mathcal{N}\\left(y|m_{n}(\\mathbf{x}), \\sigma^2_{n}(\\mathbf{x})\\right),\n",
    "$$\n",
    "where $m_{n}(\\mathbf{x})$ and $\\sigma^2_{n}(\\mathbf{x})$ are the predictive mean and variance respectively.\n",
    "Here is the code for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "# The kernel we use\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "# Assuming that we know there is no measurement noise:\n",
    "gpr.likelihood.variance.constrain_fixed(1e-16)\n",
    "# You can evaluate the predictive distribution anywhere:\n",
    "m, sigma2 = gpr.predict(x[:, None])\n",
    "# And you can visualize the results as follows\n",
    "# Standard deviation\n",
    "sigma = np.sqrt(sigma2)\n",
    "# Lower quantile\n",
    "l = m - 1.96 * sigma\n",
    "u = m + 1.96 * sigma\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "plt.plot(x, f(x), 'r--', linewidth=2, label='True function')\n",
    "ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='Observations')\n",
    "ax.plot(x, m, label='GP mean')\n",
    "ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25,\n",
    "                label='GP 95% pred. int.')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum upper interval is defined to be:\n",
    "$$\n",
    "a_n(\\mathbf{x}) = \\mu_n(\\mathbf{x}) + \\psi \\sigma_n(\\mathbf{x}),\n",
    "$$\n",
    "for some $\\psi \\ge 0$.\n",
    "Note that here we are using the predictive mean and variance.\n",
    "The parametr $\\psi$ controls how much emphasis you put on exploitation and exploration.\n",
    "The choice $\\psi = 0$ is full-on exploitation. You are just looking at the predictive mean.\n",
    "The greater $\\psi$ is, the more emphasis you put on the predictive standard deviation, i.e., the more you try to explore.\n",
    "Okay, so the information acquisition function depends only on the posterior mean, variance, and that parameter $\\psi$.\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mui(m, sigma, ymax, psi=1.96):\n",
    "    return m + psi * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write code that carries out Bayesian global optimization for using the maximum upper interval as the information acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mui(psi=0.):\n",
    "    fig, ax = plt.subplots(dpi=100)\n",
    "    ax.set_title('$\\psi={0:1.2f}$'.format(psi))\n",
    "    ax.plot(X, Y, 'kx', markersize=10, markeredgewidth=2, label='Observations')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.plot(x, m, label='GP mean')\n",
    "    ax.fill_between(x, l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25,\n",
    "                    label='GP 95% pred. int.')\n",
    "    ax.plot(x, f(x), 'r--', label='True function')\n",
    "    plt.legend(loc='best')\n",
    "    af_values = mui(m, sigma, Y.max(), psi)\n",
    "    next_id = np.argmax(af_values)\n",
    "    next_x = x[next_id]\n",
    "    af_max = af_values[next_id]\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x, af_values, color=sns.color_palette()[1])\n",
    "    ax2.set_ylabel('Maximum Upper Interval', color=sns.color_palette()[1])\n",
    "    plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "    ax2.plot(next_x * np.ones(100), np.linspace(0, af_max, 100), color=sns.color_palette()[1],\n",
    "         linewidth=1)\n",
    "\n",
    "from ipywidgets import interactive, interact_manual\n",
    "    \n",
    "interactive(plot_mui, psi=(0., 4., 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ Experiment with different values of $\\psi$.\n",
    "+ When do you get exploration?\n",
    "+ When do you get exploitation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian global optimization with the maximum upper interval\n",
    "\n",
    "Let's now run the Bayesian global optimization algorithm using the maximum upper interval as the information acquisition function.\n",
    "For convenience, I have written the following generic code for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize(f, gpr, X_design, alpha, psi=0., max_it=6):\n",
    "    \"\"\"\n",
    "    Optimize f using a limited number of evaluations.\n",
    "    \n",
    "    :param f:        The function to optimize.\n",
    "    :param gpr:      A Gaussian process model to use for representing our state of knowldege.\n",
    "    :param X_design: The set of candidate points for identifying the maximum.\n",
    "    :param alpha:    The acquisition function.\n",
    "    :param psi:      The parameter value for the acquisition function (not used for EI).\n",
    "    :param max_it:   The maximum number of iterations.\n",
    "    \"\"\"\n",
    "    af_all = []\n",
    "    for count in range(max_it):\n",
    "        m, sigma2 = gpr.predict(X_design)\n",
    "        sigma = np.sqrt(sigma2)\n",
    "        l = m - 1.96 * sigma\n",
    "        u = m + 1.96 * sigma\n",
    "        af_values = alpha(m, sigma, gpr.Y.max(), psi=psi)\n",
    "        i = np.argmax(af_values)\n",
    "        X = np.vstack([gpr.X, X_design[i:(i+1), :]])\n",
    "        y = np.vstack([gpr.Y, [f(X_design[i, :])]])\n",
    "        gpr.set_XY(X, y)\n",
    "        # Uncomment the following to optimize the hyper-parameters\n",
    "        # gpr.optimize()\n",
    "        af_all.append(af_values[i])\n",
    "        fig, ax = plt.subplots(dpi=100)\n",
    "        ax.plot(gpr.X, gpr.Y, 'kx', markersize=10, markeredgewidth=2)\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$y$')\n",
    "        ax.plot(x, m)\n",
    "        ax.fill_between(X_design.flatten(), l.flatten(), u.flatten(), color=sns.color_palette()[0], alpha=0.25)\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(X_design, af_values, color=sns.color_palette()[1])\n",
    "        plt.setp(ax2.get_yticklabels(), color=sns.color_palette()[1])\n",
    "        ax2.set_ylabel('acquisition function', color=sns.color_palette()[1])\n",
    "        ax2.plot(X_design[i, :] * np.ones(100), np.linspace(0, af_values[i], 100), color=sns.color_palette()[1],\n",
    "                 linewidth=1)\n",
    "    return af_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code accepts the information acquisition function as an input.\n",
    "Here is how you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the initial statistical model\n",
    "k = GPy.kern.RBF(1, lengthscale=0.15, variance=4.)\n",
    "gpr = GPy.models.GPRegression(X[:, None], Y[:, None], k)\n",
    "gpr.likelihood.variance.constrain_fixed(1e-16)\n",
    "\n",
    "# Run the algorithm\n",
    "af_all = maximize(f, gpr, x[:, None], alpha=mui, psi=1.96, max_it=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ Repeat the main algorithm using MUI for a $\\psi$ that exploits. Does the method converge?\n",
    "+ Repeat the main algorithm using MUI for a $\\psi$ that explores. Does the method converge?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
