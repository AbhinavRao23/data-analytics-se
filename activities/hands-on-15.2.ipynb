{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 15.2: Automatic relevance determination\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To demonstrate how automatic relevance determination can be used to select which features to keep.\n",
    "\n",
    "## Example (Quadratic)\n",
    "\n",
    "As before, let's start with our familiar synthetic ecxample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations we have\n",
    "num_obs = 10\n",
    "x = -1.0 + 2 * np.random.rand(num_obs)\n",
    "w0_true = -0.5\n",
    "w1_true = 1.0\n",
    "w2_true = 2.0\n",
    "sigma_true = 0.1\n",
    "y = w0_true + w1_true * x + w2_true * x ** 2 + sigma_true * np.random.randn(num_obs)\n",
    "# Let's plot the data\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also copy-paste the code for creating design matrices for the three generalized linear models we have considered so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_design_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Returns the polynomial design matrix of ``degree`` evaluated at ``x``.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    # Start with an empty list where we are going to put the columns of the matrix\n",
    "    cols = []\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(degree+1):\n",
    "        cols.append(x ** i)\n",
    "    return np.hstack(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to implement the automatic relevance determination from scratch. Instead we are going to use the implementation in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression).\n",
    "Here is how you do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Bayesian linear regression class:\n",
    "from sklearn.linear_model import ARDRegression\n",
    "# Select polynomial degree and get design matrix\n",
    "degree = 6\n",
    "# Build the design matrix\n",
    "Phi = get_polynomial_design_matrix(x[:, None], degree)\n",
    "# Do not normalize. ARDRegression seems to have a bug when you do.\n",
    "model = ARDRegression(normalize=False, fit_intercept=False).fit(Phi, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can get the noise variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn optimizes the precision of the noise which is the inverse of the variance.\n",
    "# It calls it alpha_.\n",
    "sigma = np.sqrt(1.0 / model.alpha_)\n",
    "print('sigma = {0:1.2f}'.format(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can get the precision corresponding to each weight.\n",
    "Remember, that very high precision means that the corresponding basis function can be safely removed from your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn optimizes calls our alpha lambda_:\n",
    "alpha = model.lambda_\n",
    "print(alpha)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.bar(np.arange(degree+1), alpha)\n",
    "ax.set_xlabel('Feature id $j$')\n",
    "ax.set_ylabel(r'$\\alpha_j$')\n",
    "ax.set_title('$N={0:d}$'.format(num_obs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that ARD tells us that we don't have to include the 4th, 5th, and 6th degree monomials.\n",
    "\n",
    "Now, let's get the postserior mean of the weights.\n",
    "Notice that the weights corresponding to 4th, 5th, and 6th degree polynomials are very close to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The posterior mean of the weights is here (this is for the normalized features, however)\n",
    "m_norm = model.coef_\n",
    "print(m_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the posterior covariance of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The posterior covariance matrix for the weights is here (also for the normalized features)\n",
    "S_norm = model.sigma_\n",
    "print(S_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the marginal posterior for each weight.\n",
    "Notice how 4th, 5th, and 6th are centered at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "# plot the posterior of all the weights\n",
    "ww = np.linspace(-3.0, 3.0, 200)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "for j in range(S_norm.shape[0]):\n",
    "    wj_post = st.norm(loc=m_norm[j], scale=np.sqrt(S_norm[j, j]))\n",
    "    ax.plot(ww, wj_post.pdf(ww), label=r'$p(w_{{{0:d}}}|x_{{1:N}}, y_{{1:N}})$'.format(j))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the posterior predictive separating aleatory and epistemic uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(-1, 1, 100)\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "yy_mean, yy_measured_std = model.predict(Phi_xx, return_std=True)\n",
    "yy_std = np.sqrt(yy_measured_std ** 2 - sigma**2)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(xx, yy_mean, 'r')\n",
    "# Epistemic lower bound\n",
    "yy_le = yy_mean - 2.0 * yy_std\n",
    "# Epistemic upper bound\n",
    "yy_ue = yy_mean + 2.0 * yy_std\n",
    "# Epistemic + aleatory lower bound\n",
    "yy_lae = yy_mean - 2.0 * yy_measured_std\n",
    "# Episemic + aleatory upper bound\n",
    "yy_uae = yy_mean + 2.0 * yy_measured_std\n",
    "ax.fill_between(xx, yy_le, yy_ue, color='red', alpha=0.25)\n",
    "ax.fill_between(xx, yy_lae, yy_le, color='green', alpha=0.25)\n",
    "ax.fill_between(xx, yy_ue, yy_uae, color='green', alpha=0.25)\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'kx', label='Observed data')\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx + w2_true * xx ** 2\n",
    "# overlay the true \n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take samples from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "# These are the unscaled features:\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "\n",
    "# Now let's put together the posterior of the weights\n",
    "import scipy.stats as st\n",
    "w_post = st.multivariate_normal(mean=m_norm, cov=S_norm)\n",
    "# If you get an error because the covariance matrix is singular, add something small\n",
    "# the diagonal. The covariance matrix is always positive definite (and non-singular)\n",
    "# but it may have eigenvalues that are so close to zero that the numerical algorithms find\n",
    "# them to be slightly negative. This is an artifact of the floating point precision.\n",
    "# Comment the line above and uncomment the line below to fix the problem.\n",
    "# w_post = st.multivariate_normal(mean=m_norm, cov=S_norm + 1e-6 * np.eye(S_norm.shape[0]))\n",
    "\n",
    "# Let's take a plot some posterior samples\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "for _ in range(10):\n",
    "    w_sample = w_post.rvs()\n",
    "    yy_sample = np.dot(Phi_xx, w_sample)\n",
    "    ax.plot(xx, yy_sample, 'r', lw=0.5)\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'kx', label='Observed data')\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx + w2_true * xx ** 2\n",
    "# overlay the true \n",
    "ax.plot(xx, yy_true, label='True response surface')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ Rerun with a smaller number of observations, say $N=5$. What happens to the epistemic uncertainty?\n",
    "+ Rerun with a very small number of observations, say $N=2$. What happens then? (The step that samples from the posterior may not work as expected. Please look at the comment in the code to fix the problem.)\n",
    "+ Rerun everything with a higher degree polynomial. Try $4$, $8$, and $16$.\n",
    "Notice that the fit remains good in between but the way you extrapolate changes. Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
