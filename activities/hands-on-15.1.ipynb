{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 15.1: Evidence approximation\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To demonstrate one can estimate the hyper-parameters of Bayesian model using the evidence approximation.\n",
    "\n",
    "## Example (Quadratic)\n",
    "\n",
    "Let's generate some synthetic data from a quadratic function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations we have\n",
    "num_obs = 10\n",
    "x = -1.0 + 2 * np.random.rand(num_obs)\n",
    "w0_true = -0.5\n",
    "w1_true = 2.0\n",
    "w2_true = 2.0\n",
    "sigma_true = 0.1\n",
    "y = w0_true + w1_true * x + w2_true * x ** 2 + sigma_true * np.random.randn(num_obs)\n",
    "# Let's plot the data\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also copy paste the code for creating design matrices for the three generalized linear models we have considered so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_design_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Returns the polynomial design matrix of ``degree`` evaluated at ``x``.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    # Start with an empty list where we are going to put the columns of the matrix\n",
    "    cols = []\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(degree+1):\n",
    "        cols.append(x ** i)\n",
    "    return np.hstack(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to implement the evidence approximation from scratch. Instead we are going to use the implementation in [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge).\n",
    "Scikit-learn makes some default assumptions about the hyper-priors of $\\sigma^2$ and $\\alpha$.\n",
    "You can change these assumptions if you wish (they assign a Gamma distribution to the inverse of these parameters).\n",
    "However, these hyper-priors are generated under the assumption that the input-output pairs have been *normalized* before fitting.\n",
    "To normalize the data, we subtract from the features and the outputs their empirical mean and we divide by their L2 norm.\n",
    "Then you do regression between normalized features and outputs and you make sure that you correctly transform the results of your analysis back to the original ones.\n",
    "Scikit-learn makes all this super easy as you just need to pass a single argument that says that you want to normalize.\n",
    "Here is how you do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Bayesian linear regression class:\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "# Select polynomial degree and get design matrix\n",
    "degree = 3\n",
    "# Build the design matrix\n",
    "Phi = get_polynomial_design_matrix(x[:, None], degree)\n",
    "# Train the model (notice the normalize=True argument)\n",
    "# Also, the fit_intercept=False is required so that the model does not\n",
    "# fit a constant offset (the intercept). This is included in our models\n",
    "# as the first basis function phi_1(x) = 1.\n",
    "model = BayesianRidge(normalize=True, fit_intercept=False).fit(Phi, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to get the $\\sigma$ that scikit-learn finds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn optimizes the precision of the noise which is the inverse of the variance.\n",
    "# It calls it alpha_.\n",
    "sigma = np.sqrt(1.0 / model.alpha_)\n",
    "print('sigma = {0:1.2f}'.format(sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to get the $\\alpha$ that scikit-learn finds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn optimizes the inverse of our alpha (the variance of the weights), which\n",
    "# it calls lambda_.\n",
    "alpha = np.sqrt(1.0 / model.lambda_)\n",
    "print('alpha = {0:1.2f}'.format(alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior mean of the weights (for the normalized features) is accessible here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The posterior mean of the weights is here (this is for the normalized data, however)\n",
    "m_norm = model.coef_\n",
    "print(m_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior covariance matrix of the weights (for the normalized features) is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The posterior covariance matrix for the weights is here (also for the normalized data)\n",
    "S_norm = model.sigma_\n",
    "print(S_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make predictions separating aleatory and epistemic uncertainty just like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some points on which to evaluate the point predictive:\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "# The features on these points (you don't have to normalize them, the ``model`` will\n",
    "# do it internally)\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "# Use the model to get the predictive mean and standard deviation:\n",
    "yy_mean, yy_measured_std = model.predict(Phi_xx, return_std=True)\n",
    "# Separate the epistemic uncertainty\n",
    "yy_std = np.sqrt(yy_measured_std ** 2 - sigma**2)\n",
    "# Plot\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(xx, yy_mean, 'r')\n",
    "# Epistemic lower bound\n",
    "sigma2 = 0.1 ** 2\n",
    "yy_le = yy_mean - 2.0 * yy_std\n",
    "# Epistemic upper bound\n",
    "yy_ue = yy_mean + 2.0 * yy_std\n",
    "# Epistemic + aleatory lower bound\n",
    "yy_lae = yy_mean - 2.0 * yy_measured_std\n",
    "# Episemic + aleatory upper bound\n",
    "yy_uae = yy_mean + 2.0 * yy_measured_std\n",
    "ax.fill_between(xx, yy_le, yy_ue, color='red', alpha=0.25)\n",
    "ax.fill_between(xx, yy_lae, yy_le, color='green', alpha=0.25)\n",
    "ax.fill_between(xx, yy_ue, yy_uae, color='green', alpha=0.25)\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'kx', label='Observed data')\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx + w2_true * xx ** 2\n",
    "# overlay the true \n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you wanted to get the samples from the posterior?\n",
    "This requires a bit of manual work because it is not implemented in scikit-learn.\n",
    "We just need to make sure that we use the normalized features to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get mean of the features as calculated inside the model\n",
    "Phi_mean = model.X_offset_\n",
    "# Then, let's get the scales of the features as caclulated inside the model\n",
    "Phi_scale = model.X_scale_\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "# These are the unscaled features:\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "# Here is how to scale them\n",
    "Phi_xx_scaled = (Phi_xx - Phi_mean) / Phi_scale\n",
    "# Note: We did not have to do this before because scikit-learn was already doing this\n",
    "# for us\n",
    "\n",
    "# Now let's put together the posterior of the weights\n",
    "import scipy.stats as st\n",
    "w_post = st.multivariate_normal(mean=m_norm, cov=S_norm)\n",
    "# If you get an error because the covariance matrix is singular, add something small\n",
    "# the diagonal. The covariance matrix is always positive definite (and non-singular)\n",
    "# but it may have eigenvalues that are so close to zero that the numerical algorithms find\n",
    "# them to be slightly negative. This is an artifact of the floating point precision.\n",
    "# Comment the line above and uncomment the line below to fix the problem.\n",
    "# w_post = st.multivariate_normal(mean=m_norm, cov=S_norm + 1e-6 * np.eye(S_norm.shape[0]))\n",
    "\n",
    "# Let's take a plot some posterior samples\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "for _ in range(10):\n",
    "    w_sample = w_post.rvs()\n",
    "    yy_sample = np.dot(Phi_xx_scaled, w_sample)\n",
    "    ax.plot(xx, yy_sample, 'r', lw=0.5)\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'kx', label='Observed data')\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx + w2_true * xx ** 2\n",
    "# overlay the true \n",
    "ax.plot(xx, yy_true, label='True response surface')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ Rerun with a smaller number of observations, say $N=5$. What happens to the epistemic uncertainty?\n",
    "+ Rerun with a very small number of observations, say $N=2$. What happens then? (The step that samples from the posterior may not work as expected. Please look at the comment in the code to fix the problem.)\n",
    "+ Rerun everything with a higher degree polynomial. Try $4$, $8$, and $16$.\n",
    "Notice that the fit remains good in between but the way you extrapolate changes. Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
