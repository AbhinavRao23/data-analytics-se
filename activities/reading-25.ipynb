{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Activity 25 - Deep Neural Networks Continued\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Derive a loss function for binary classification\n",
    "+ Understand the mathematics of convolutional layers\n",
    "+ Understand how regularization parameters help us avoid overfitting\n",
    "+ Understand the Bayesian interpretation of regularization parameters\n",
    "+ Use data augmentation to exploit known symmetries, e.g., translation and rotation invariance, of your dataset\n",
    "+ Use early stopping to avoid overfitting\n",
    "+ Tune the network hyper-parameters using grid search and Bayesian global optimization\n",
    "+ Learn about the state-of-the-art in regularization techniques\n",
    "\n",
    "## References\n",
    "\n",
    "+ Chapters 7, 9, and 11 of https://www.deeplearningbook.org/\n",
    "+ These notes.\n",
    "\n",
    "These notes are not exhaustive. They merely provide a summary. Please consult the book chapters for the complete details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions for classification\n",
    "\n",
    "Take some features $\\mathbf{x}_{1:n}$ and some discrete targets $y_{1:n}$.\n",
    "Because the targets are discrete, we have a classification problem.\n",
    "What loss function should we use?\n",
    "Let's examine two cases: binary and multiclass classification.\n",
    "\n",
    "### Binary classification\n",
    "\n",
    "In binary classification, we use a DNN $f(\\mathbf{x};\\theta)$ with parameters $\\theta$ to model the probability that $y$ take the value $1$ by:\n",
    "$$\n",
    "p(y=1|\\mathbf{x},\\theta) = \\operatorname{sigm}(f(\\mathbf{x};\\theta)) = \\frac{\\exp\\{f(\\mathbf{x};\\theta)\\}}{1 + \\exp\\{f(\\mathbf{x};\\theta)\\}}.\n",
    "$$\n",
    "Remember that the sigmoid function takes the scalar $f(\\mathbf{x};\\theta)$ and maps it on $[0,1]$ so that we get a probability.\n",
    "From the obvious rule of probability, we get that:\n",
    "$$\n",
    "p(y=0|\\mathbf{x},\\theta) = 1 - p(y=1|\\mathbf{x},\\theta) = 1 - \\operatorname{sigm}(f(\\mathbf{x};\\theta)).\n",
    "$$\n",
    "So, for an arbitrary $y$ (either 0 or 1), we can write:\n",
    "$$\n",
    "p(y|\\mathbf{x},\\theta) = \\left[\\operatorname{sigm}(f(\\mathbf{x};\\theta))\\right]^y\n",
    "\\left[1-\\operatorname{sigm}(f(\\mathbf{x};\\theta))\\right]^{1-y}.\n",
    "$$\n",
    "This is a nice trick because it activates the right term based on what $y$ is.\n",
    "\n",
    "Now that we have specified the likelihood of a single observation, the likelihood of the entire dataset is:\n",
    "$$\n",
    "p(y_{1:n}|\\mathbf{x}_{1:n},\\theta) = \\prod_{i=1}^n p(y_i|\\mathbf{x},\\theta).\n",
    "$$\n",
    "We are almost done.\n",
    "The idea is to train the network by maximizing the log likelihood, which is the same as minimizing the following loss function:\n",
    "\\begin{split}\n",
    "L(\\theta) &= -\\log p(y_{1:n}|\\mathbf{x}_{1:n},\\theta)\\\\\n",
    "&= -\\sum_{i=1}^n \\left\\{y_i \\log \\operatorname{sigm}(f(\\mathbf{x}_i;\\theta))\n",
    "+ (1-y_i)\\log \\left[1-\\operatorname{sigm}(f(\\mathbf{x}_i;\\theta))\\right]\n",
    "\\right\\}.\n",
    "\\end{split}\n",
    "This loss function is known as the *cross entropy* loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "Now assume that $y$ can take $K$ different values ranging from $0$ to $K-1$.\n",
    "We need to model the probability that $y=k$ given $\\mathbf{x}$.\n",
    "To do this, we introduce a DNN $\\mathbf{f}(\\mathbf{x};\\theta)$ with parameters $\\theta$ and $K$ outputs:\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x};\\theta) = \\left(f_0(\\mathbf{x};\\theta),\\dots,f_{K-1}(\\mathbf{x};\\theta)\\right).\n",
    "$$\n",
    "However, $\\mathbf{f}(\\mathbf{x})$ is just a bunch of $K$ scalars.\n",
    "We need to turn it into a $K$-dimensional probability vector.\n",
    "To achieve this, we define:\n",
    "$$\n",
    "p(y=k|\\mathbf{x},\\theta) = \\operatorname{softmax}_k(\\mathbf{f}(\\mathbf{x};\\theta))\n",
    ":= \\frac{\\exp\\left\\{f_k(\\mathbf{x};\\theta)\\right\\}}{\\sum_{k'=0}^{K-1}\\exp\\left\\{f_{k'}(\\mathbf{x};\\theta)\\right\\}}.\n",
    "$$\n",
    "So, the role of the softmax is dule. First, to turn the scalars to positive numbers and, second, to normalize them.\n",
    "\n",
    "For an arbitrary $y$, we can just write:\n",
    "$$\n",
    "p(y|\\mathbf{x},\\theta)) = \\operatorname{softmax}_y(\\mathbf{f}(\\mathbf{x};\\theta)).\n",
    "$$\n",
    "So, the likelihood of the dataset is:\n",
    "$$\n",
    "p(y_{1:n}|\\mathbf{x}_{1:n},\\theta) = \\prod_{i=1}^n p(y_i|\\mathbf{x},\\theta).\n",
    "$$\n",
    "Therefore, the loss function we should be minimizing is:\n",
    "\\begin{split}\n",
    "L(\\theta) &= -\\log p(y_{1:n}|\\mathbf{x}_{1:n},\\theta)\\\\\n",
    "&= -\\sum_{i=1}^n \\log \\left[\\operatorname{softmax}_{y_i}(\\mathbf{f}(\\mathbf{x}_i;\\theta))\\right].\n",
    "\\end{split}\n",
    "This is also a called *cross entropy* loss, but for multiclass classification.\n",
    "Sometimes, it is called the *softmax cross entropy* loss.\n",
    "Now all these names are not really important.\n",
    "What is important is to understand how these losses are derived from maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization \n",
    "\n",
    "DNNs are extremely flexible. If "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
