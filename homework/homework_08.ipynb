{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8\n",
    "\n",
    "**Due: 11/13/2020 on gradescope**\n",
    "\n",
    "## References\n",
    "\n",
    "+ Lectures 21-23 (inclusive).\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you can either:\n",
    "    \n",
    "    - Type the answer using the built-in latex capabilities. In this case, simply export the notebook as a pdf and upload it on gradescope; or\n",
    "    - You can print the notebook (after you are done with all the code), write your answers by hand, scan, turn your response to a single pdf, and upload on gradescope.\n",
    "\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally.\n",
    "\n",
    "**Note**: Please match all the pages corresponding to each of the questions when you submit on gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "import scipy.stats as st\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)\n",
    "try:\n",
    "  import GPy\n",
    "except:\n",
    "  _=!pip install GPy\n",
    "  import GPy\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Defining priors on function spaces\n",
    "\n",
    "In this problem we are going to explore further how Gaussian processes can be used to define probability measures over function spaces.\n",
    "To this end, assume that there is a 1D function, call if $f(x)$, which we do not know.\n",
    "For simplicity, assume that $x$ takes values in $[0,1]$.\n",
    "We will employ Gaussian process regression to encode our state of knowledge about $f(x)$ and sample some possibilities for it.\n",
    "For each of the cases below:\n",
    "+ assume that $f\\sim \\operatorname{GP}(m, k)$ and pick a mean ($m(x)$) and a covariance function $f(x)$ that match the provided information.\n",
    "+ write code that samples a few times (up to five) the values of $f(x)$ at a 100 equidistant points between 0 and 1.\n",
    "\n",
    "### Part A - Super smooth function with known length scale\n",
    "\n",
    "Assume that you hold the following beliefs\n",
    "+ You know that $f(x)$ has as many derivatives as you want and they are all continuous\n",
    "+ You don't know if $f(x)$ has a specific trend.\n",
    "+ You think that $f(x)$ has \"wiggles\" that are approximatly of size $\\Delta x=0.1$.\n",
    "+ You think that $f(x)$ is between -4 and 4.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**I am doing this for you so that you have a concrete example of what is requested.**\n",
    "\n",
    "The mean function should be:\n",
    "$$\n",
    "m(x) = 0.\n",
    "$$\n",
    "The covariance function should be a squared exponential:\n",
    "$$\n",
    "k(x,x') = s^2\\exp\\left\\{-\\frac{(x-x')^2}{2\\ell^2}\\right\\},\n",
    "$$\n",
    "with variance:\n",
    "$$\n",
    "s^2 = k(x,x) = \\mathbb{V}[f(x)] = 4,\n",
    "$$\n",
    "and lengthscale $\\ell = 0.1$.\n",
    "We chose the variance to be 2 so that with (about) 95% probability the values of $f(x)$ are between -4 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the covariance function\n",
    "k = GPy.kern.RBF(1)\n",
    "k.lengthscale = 0.1\n",
    "k.variance = 4.0\n",
    "# Sample\n",
    "xs = np.linspace(0, 1, 100)\n",
    "# The mean function at xs\n",
    "ms = np.zeros(xs.shape)\n",
    "# Find the covariance matrix. You need to add a small number\n",
    "# to the diagonal to ensure numerical stability\n",
    "nugget = 1e-6\n",
    "K = k.K(xs[:, None]) + nugget * np.eye(xs.shape[0])\n",
    "# A multivariate normal that can be used to sample the function values\n",
    "F = st.multivariate_normal(mean=ms.flatten(), cov=K)\n",
    "# Take the function samples\n",
    "f_samples = F.rvs(size=5)\n",
    "# Plot the samples\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(xs, f_samples.T, 'r', lw=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B - Super smooth function with known ultra small length scale\n",
    "\n",
    "Assume that you hold the following beliefs\n",
    "+ You know that $f(x)$ has as many derivatives as you want and they are all continuous\n",
    "+ You don't know if $f(x)$ has a specific trend.\n",
    "+ You think that $f(x)$ has \"wiggles\" that are approximatly of size $\\Delta x=0.05$.\n",
    "+ You think that $f(x)$ is between -3 and 3.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C - Continuous function with known length scale\n",
    "\n",
    "Assume that you hold the following beliefs\n",
    "+ You know that $f(x)$ is continuous, nowhere differentiable.\n",
    "+ You don't know if $f(x)$ has a specific trend.\n",
    "+ You think that $f(x)$ has \"wiggles\" that are approximatly of size $\\Delta x=0.1$.\n",
    "+ You think that $f(x)$ is between -5 and 5.\n",
    "\n",
    "Hint: Use ``GPy.kern.Exponential``.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D - Smooth periodic function with known length scale\n",
    "\n",
    "Assume that you hold the following beliefs\n",
    "+ You know that $f(x)$ is smooth.\n",
    "+ You know that $f(x)$ is periodic with period 0.1.\n",
    "+ You don't know if $f(x)$ has a specific trend.\n",
    "+ You think that $f(x)$ has \"wiggles\" that are approximatly of size $\\Delta x=0.5$ of the period.\n",
    "+ You think that $f(x)$ is between -5 and 5.\n",
    "\n",
    "Hint: Use ``GPy.kern.StdPeriodic``.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E - Smooth periodic function with known length scale\n",
    "\n",
    "Assume that you hold the following beliefs\n",
    "+ You know that $f(x)$ is smooth.\n",
    "+ You know that $f(x)$ is periodic with period 0.1.\n",
    "+ You don't know if $f(x)$ has a specific trend.\n",
    "+ You think that $f(x)$ has \"wiggles\" that are approximatly of size $\\Delta x=0.1$ of the period (**the only thing that is different compared to D**).\n",
    "+ You think that $f(x)$ is between -5 and 5.\n",
    "\n",
    "Hint: Use ``GPy.kern.StdPeriodic``.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F - The sum of two functions\n",
    "\n",
    "Assume that you hold the following beliefs\n",
    "+ You know that $f(x) = f_1(x) + f_2(x)$, where:\n",
    "    - $f_1(x)$ is smooth with variance 2 and lengthscale 0.5\n",
    "    - $f_2(x)$ is continuous, nowhere differentiable with variance 0.1 and lengthscale 0.1\n",
    "\n",
    "Hint: Use must create a new covariance function that is the sum of two other covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G - The product of two functions\n",
    "\n",
    "Assume that you hold the following beliefs\n",
    "+ You know that $f(x) = f_1(x)f_2(x)$, where:\n",
    "    - $f_1(x)$ is smooth, periodic (period = 0.1), lengthscale 0.1 (relative to the period), and variance 2.\n",
    "    - $f_2(x)$ is smooth with lengthscale 0.5 and variance 1.\n",
    "\n",
    "Hint: Use must create a new covariance function that is the product of two other covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "The National Oceanic and Atmospheric Administration (NOAA) has been measuring the levels of atmospheric CO2 at the Mauna Loa, Hawaii. The measurements start on March 1958 and go all the way to Janurary 2016.\n",
    "The data can be found [here](http://www.esrl.noaa.gov/gmd/ccgg/trends/data.html).\n",
    "The Python script below, downloads and plots the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/mauna_loa_co2.txt'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('mauna_loa_co2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "t = data[:, 2][:, None]  #time (in decimal dates)\n",
    "y = data[:, 4][:, None]  #CO2 level (mole fraction in dry air, micromol/mol, abbreviated as ppm)\n",
    "fig, ax = plt.subplots(1, figsize = (15, 10), dpi=100)\n",
    "ax.plot(t, y, '.')\n",
    "ax.set_xlabel('$t$ (year)', fontsize = 16)\n",
    "ax.set_ylabel('$y$ (CO2 level in ppm)', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we observe a steady growth of CO2 levels. The wiggles correspond to seasonal changes. Since the vast majority of the population inhabits the Northen hemisphere, fuel consumption goes up during the Northen winters and CO2 emissions follow. Our goal is to study this dataset with Gaussian process regression. Specifically we would like to predict the evolution of the CO2 levels from Feb 2018 to Feb 2028 and quantify our uncertainty about this prediction.\n",
    "\n",
    "It's always a good idea to work with at scaled version of the inptus and the outputs. We are going to scale the times as follows:\n",
    "$$\n",
    "t_s = t - t_{\\min}.\n",
    "$$\n",
    "So, time is still in fractional years, but we start counting at zero instead of 1950.\n",
    "We scale the $y$'s as:\n",
    "$$\n",
    "y_s = \\frac{y - y_{\\min}}{y_{\\max}-y_{\\min}}.\n",
    "$$\n",
    "This takes all the $y$ between $0$ and $1$.\n",
    "Here is how the scaled data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_s = t - t.min()\n",
    "y_s = (y - y.min()) / (y.max() - y.min())\n",
    "fig, ax = plt.subplots(1, figsize = (15, 10), dpi=100)\n",
    "ax.plot(t_s, y_s, '.')\n",
    "ax.set_xlabel('$t_s$ (Scaled year)', fontsize = 16)\n",
    "ax.set_ylabel('$y_s (Scaled CO2 level)$', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, just work with the scaled data as you develop your model.\n",
    "Scale back to the original units for your final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Naive approach\n",
    "\n",
    "Use a zero mean Gaussian process with a squared exponential covariance function to fit the data and make the required prediction (ten years after the last observation).\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "naive_model = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict everything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = np.linspace(0, t_s.max() + 10, 200)[:, None]\n",
    "ys, vs = naive_model.predict(tss)\n",
    "ls = ys - 1.96 * np.sqrt(vs)\n",
    "us = ys + 1.96 * np.sqrt(vs)\n",
    "fig, ax = plt.subplots(1, figsize = (15, 10), dpi=100)\n",
    "ax.plot(tss, ys, color='blue', label='Posterior mean')\n",
    "ax.fill_between(tss.flatten(), ls.flatten(), us.flatten(), color='blue', alpha=0.25)\n",
    "ax.plot(t_s, y_s, '.', label='Scaled observed data')\n",
    "ax.set_xlabel('$t_s$ (Scaled year)', fontsize = 16)\n",
    "ax.set_ylabel('$y_s$ (Scaled CO2 level)', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the squared exponential covariance caputes the long terms, but it fails to capture the seasonal fluctuations. As a matter of fact the seasonabl fluctions as treated as noise. This is clearly false. How can we fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Improving the prior covariance\n",
    "\n",
    "Now use the ideas of Problem 1, to come up with a covariance function that is exhibits the following characteristics clearly visible in the data (call $f(x)$ the scaled CO2 level.\n",
    "+ $f(x)$ is smooth\n",
    "+ $f(x)$ has a clear trend with a multi-year lengthscale (it is also an increasing trend, but we are not going to impose this)\n",
    "+ $f(x)$ has seasonal fluctations with a period of one year\n",
    "+ $f(x)$ exhibits small fluctiations within its period.\n",
    "\n",
    "Use summation and multiplication of simple covariance functions to create a covariance function that exhibits these trends.\n",
    "Sample a few times from it.\n",
    "\n",
    "Hint: Do not attempt to fit the data in any way. Just try to find a covariance function that has the right features. We also do not care about getting the parameters 100% right at this point. The parameters will be optimized later.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "There is more than one way to do this.\n",
    "We are going to assume that:\n",
    "$$\n",
    "f(x) = f_1(x) (f_2(x) + f_3(x)),\n",
    "$$\n",
    "where $f_1(x)$ is the ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Predicting the future\n",
    "\n",
    "Use a zero mean Gaussian process with the covariance function you picked above to do Gaussian process regression\n",
    "and make the required prediction (ten years after the last observation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - Bayesian information criterion\n",
    "\n",
    "As we have seen in earlier lectures, the Bayesian informationc criterion (BIC), see [this](https://en.wikipedia.org/wiki/Bayesian_information_criterion), can bse used to compare two models.\n",
    "The criterion says that one should:\n",
    "+ fit the models with maximum likelihood,\n",
    "+ and compute the quantity:\n",
    "$$\n",
    "\\text{BIC} = d\\ln(n) - 2\\ln(\\hat{L}),\n",
    "$$\n",
    "where $d$ is the number of model parameters, and $\\hat{L}$ the maximum likelihood.\n",
    "+ pick the model with the smallest BIC.\n",
    "\n",
    "Use BIC to show that the model you constructed in Part C is indeed better than the naïve model of Part A.\n",
    "\n",
    "Hint: Do a ``help(GPy.models.GPRegression)`` and you will find a way to get both the number of parameters and the log likelihood. Ask on piazza if you can't find it - or Google it.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E - Plot samples from the posterior Gaussian process\n",
    "\n",
    "Using the model of Part C, plot 5 samples from the posterior Gaussian process between 2018 and 2028.\n",
    "\n",
    "Hint: You need to use ``GPy.models.GPRegression.posterior_samples_f``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Using Bayesian Global optimization to calibrate an expensive physical model\n",
    "\n",
    "This is Example 3.1 of [(Tsilifis, 2014)](http://arxiv.org/abs/1410.5522).\n",
    "\n",
    "Consider the catalytic\n",
    "conversion of nitrate ($\\mbox{NO}_3^-$) to nitrogen ($\\mbox{N}_2$) and other\n",
    "by-products by electrochemical means.\n",
    "The mechanism that is followed is complex and not well understood.\n",
    "The experiment of [(Katsounaros, 2012)](http://www.sciencedirect.com/science/article/pii/S0013468612005208) confirmed the\n",
    "production of nitrogen ($\\mbox{N}_2$), ammonia\n",
    "($\\mbox{NH}_3$), and nitrous oxide ($\\mbox{N}_2\\mbox{O}$) as final products\n",
    "of the reaction, as well as the intermediate production of nitrite ($\\mbox{NO}_2^-$).\n",
    "The data are reproduced in [Comma-separated values](https://en.wikipedia.org/wiki/Comma-separated_values) (CSV) and stored in\n",
    "[catalysis.csv](https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/catalysis.csv).\n",
    "The time is measured in minutes and the conentrations are measured in $\\mbox{mmol}\\cdot\\mbox{L}^{-1}$.\n",
    "Let's load the data into this notebook using the [Pandas](http://pandas.pydata.org) Python module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/catalysis.csv'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "catalysis_data = pd.read_csv('catalysis.csv')\n",
    "catalysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalysis_data.plot(style='s', x=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory of catalytic reactions guarantees that the total mass must be conserved.\n",
    "However, this is not the case in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalysis_data.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This inconsistency suggests the existence of an intermediate unobserved reaction product X.\n",
    "[(Katsounaros, 2012)](http://www.sciencedirect.com/science/article/pii/S0013468612005208) suggested that the following reaction path shown in the following figure.\n",
    "\n",
    "The dynamical system associated with the reaction is:\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\frac{d \\left[\\mbox{NO}_3^-\\right]}{dt} &= -k_1\\left[\\mbox{NO}_3^-\\right], \\\\\n",
    "\\frac{d\\left[\\mbox{NO}_2^-\\right]}{dt} &= k_1\\left[\\mbox{NO}_3^-\\right] - (k_2 + k_4 +\n",
    "k_5)[\\mbox{NO}_2^-], \\\\\n",
    "\\frac{d \\left[\\mbox{X}\\right]}{dt} &= k_2 \\left[\\mbox{NO}_2^-\\right] - k_3 [X],\\\\\n",
    "\\frac{d \\left[\\mbox{N}_2\\right]}{dt} &= k_3 \\left[\\mbox{X}\\right], \\\\\n",
    "\\frac{d \\left[\\mbox{NH}_3\\right]}{dt} &= k_4 \\left[\\mbox{NO}_2^-\\right],\\\\\n",
    "\\frac{d \\left[\\mbox{N}_2O\\right]}{dt} &= k_5 \\left[\\mbox{NO}_2^-\\right],\n",
    "\\end{array}\n",
    "$$\n",
    "where $[\\cdot]$ denotes the concentration of a quantity, and\n",
    "$k_i > 0$, $i=1,...5$ are the *kinetic rate constants*.\n",
    "\n",
    "In this problem, I am going to guide you through the calibration of the parameters of this model so that we match the observations.\n",
    "These problems are also known as *inverse problems*.\n",
    "The problem can, and should, be formulated in a Bayesian way.\n",
    "However, in this homework problem we are going to do it using a classical loss-minimization approach.\n",
    "We will discuss the Bayesian approach for calibrating the same model in a later lecture.\n",
    "\n",
    "Before you proceed, please read a little bit about the \"classical theory of inverse problems:\"\n",
    "\n",
    "### Classical theory of inverse problems\n",
    "\n",
    "Suppose that you have a model (any model really) that predicts a quantity of interest.\n",
    "Let's assume that this model has parameters that you do not know.\n",
    "These parameters could be simple scalars (mass, spring constant, dumping coefficients, etc.) or it could be also be functions (initial conditions, boundary values, spatially distributed constitutive relations, etc.)\n",
    "Let's denote all these parameters with the vector $x$.\n",
    "Assume that:\n",
    "$$\n",
    "x\\in\\mathcal{X} \\subset\\mathbb{R}^d.\n",
    "$$\n",
    "Now, let's say we perform an experiment that measures a *noisy* vector:\n",
    "$$\n",
    "y\\in\\mathcal{Y}\\subset \\mathbb{R}^m.\n",
    "$$\n",
    "Assume that, you can use your model *model* to predict $y$.\n",
    "It does not matter how complicated your model is.\n",
    "It could be a system of ordinary differential or partial differential equations, or something more complicated.\n",
    "If it predicts $y$, you can always think of it as a function from the unknown parameter space $\\mathcal{X}$ to the space of $y$'s, $\\mathcal{Y}\\subset\\mathbb{R}^m$.\n",
    "That is, you can think of it as giving rise to a function:\n",
    "$$\n",
    "f :\\mathcal{X} \\rightarrow \\mathcal{Y}.\n",
    "$$\n",
    "\n",
    "The **inverse problem**, otherwise known as the **model calibration** problem is to find the ``best`` $x\\in\\mathcal{X}$ so that:\n",
    "$$\n",
    "f(x) \\approx y.\n",
    "$$\n",
    "\n",
    "### Formulation of Inverse Problems as Optimization Problems\n",
    "Saying that $f(x)\\approx y$ is not an exact mathematical statement.\n",
    "What does it really mean for $f(x)$ to be close to $y$?\n",
    "To quantify this, let us introduce a *loss metric*:\n",
    "$$\n",
    "\\ell: \\mathcal{Y}\\times\\mathcal{Y}\\rightarrow \\mathbb{R},\n",
    "$$\n",
    "such that $\\ell(f(x),y)$ is how much our prediction is off if we chose the input $x\\in\\mathcal{X}$.\n",
    "Equiped with this loss metric, we can formulate the mathematical problem as:\n",
    "$$\n",
    "\\min_{x\\in\\mathcal{X}} \\ell(f(x),y).\n",
    "$$\n",
    "\n",
    "#### The Square Loss\n",
    "The choice of the metric is somewhat subjective (it depends on what it means to be wrong in your problem).\n",
    "However, a very common assumption is that to take the *square loss*:\n",
    "$$\n",
    "\\ell(f(x), y) = \\frac{1}{2}\\parallel f(x) - y\\parallel_2^2 = \\frac{1}{2}\\sum_{i=1}^m\\left(f_i(x)-y_i\\right)^2.\n",
    "$$\n",
    "For this case, the inverse problem can be formulated as:\n",
    "$$\n",
    "\\min_{x\\in\\mathcal{X}}\\frac{1}{2}\\parallel f(x) - y\\parallel_2^2.\n",
    "$$\n",
    "\n",
    "#### Solution Methodologies\n",
    "We basically have to solve an optimization problem.\n",
    "For the square loss function, if $f(x)$ is linear, then you get the classic least squares problem which has a known solution.\n",
    "Otherwise, you get what is known as *generalized least squares*.\n",
    "There are many algorithms that you could use this problem.\n",
    "Several are implemented in [scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html).\n",
    "If you are able to implement your model as a simple python function, then you can use them.\n",
    "Alternatively, and this is what we are going to do here, we could use Bayesian global optimization instead.\n",
    "The absolutely, essential thing that you need to provide to these methods is the function they are optimizing, i.e.,\n",
    "$$\n",
    "L(x,y) = \\ell(f(x),y).\n",
    "$$\n",
    "\n",
    "### Back to the catalysis model\n",
    "\n",
    "Let's now formulate the calibration problem for the catalysis model.\n",
    "We proceed in several steps.\n",
    "\n",
    "#### Step 1: Making our life easier by simplifying the notation\n",
    "Note that this is actually a linear system.\n",
    "To simplify our notation, let's define:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "z_1 &:=& \\left[\\mbox{NO}_3^-\\right],\\\\\n",
    "z_2 &:=& \\left[\\mbox{NO}_2^-\\right],\\\\\n",
    "z_3 &:=& \\left[\\mbox{X}\\right],\\\\\n",
    "z_4 &:=& \\left[\\mbox{N}_2\\right],\\\\\n",
    "z_5 &:=& \\left[\\mbox{NH}_3\\right],\\\\\n",
    "z_6 &:=& \\left[\\mbox{N}_2O\\right],\n",
    "\\end{array}\n",
    "$$\n",
    "the vector:\n",
    "$$\n",
    "z = (z_1,z_2,z_3,z_4,z_5,z_6),\n",
    "$$\n",
    "and the matrix:\n",
    "$$\n",
    "A(k_1,\\dots,k_5) = \\left(\\begin{array}{cccccc}\n",
    "-k_1 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "k_1 & -(k_2+k_4+k_5) & 0 & 0 & 0 & 0\\\\\n",
    "0 & k_2 & -k_3 & 0 & 0 & 0\\\\\n",
    "0 & 0 & k_3 & 0 & 0 & 0\\\\\n",
    "0 & k_4 & 0 & 0 & 0 & 0\\\\\n",
    "0 & k_5 & 0 & 0 & 0 & 0\n",
    "\\end{array}\\right)\\in\\mathbb{R}^{6\\times 6}.\n",
    "$$\n",
    "With these definitions, the dynamical system becomes:\n",
    "$$\n",
    "\\dot{z} = A(k_1,\\dots,k_5)z,\n",
    "$$\n",
    "with initial conditions\n",
    "$$\n",
    "z(0) = z_0 = (500, 0, 0, 0, 0, 0)\\in\\mathbb{R}^6,\n",
    "$$\n",
    "read directly from the experimental data.\n",
    "What we are definitely going to need is a solver for this system.\n",
    "That's easy.\n",
    "Let's denote the solution of the system at time $t$ by:\n",
    "$$\n",
    "z(t;k_1,\\dots,k_5).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Scale the unknown parameters to your best of your abilities\n",
    "The constraints you have on your parameters, the better.\n",
    "If you do have constraints, you would have to use constrained optimization algorithms.\n",
    "The way you scale things depend on the problem.\n",
    "Here we would think as follows:\n",
    "\n",
    "+ $k_i$ has units of inverse time. It is proparly appropriate to scale it with the total time which is 180 minutes.\n",
    "So, let's just multiply $k_i$ with 180. This makes the resulting variable dimensionless:\n",
    "$$\n",
    "\\hat{x}_i = 180k_i.\n",
    "$$\n",
    "\n",
    "+ $k_i$ is positive, therefore $\\hat{x_i}$ must be positive.\n",
    "So, let's just work with the logarithm of $\\hat{x_i}$:\n",
    "$$\n",
    "x_i = \\log \\hat{x_i} = \\log 180k_i.\n",
    "$$\n",
    "\n",
    "+ define the parameter vector:\n",
    "$$\n",
    "x = (x_1,\\dots,x_5)\\in\\mathcal{X} = \\mathbb{R}^5.\n",
    "$$\n",
    "\n",
    "From now on, we will write\n",
    "$$\n",
    "A = A(x),\n",
    "$$\n",
    "for the matrix of the dynamical system, and\n",
    "$$\n",
    "z = z(t;x),\n",
    "$$\n",
    "for the solution at $t$ given that the parameters are $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Making the connection between our model and the experimental measurements\n",
    "Our experimental data include measurements of everything except $z_3$ at times six (6) time instants:\n",
    "$$\n",
    "t_j = 30j\\;\\mbox{minutes},\n",
    "$$\n",
    "$j=1,\\dots,6$.\n",
    "\n",
    "Now, let $Y\\in\\mathbb{R}^{5\\times 6}$ be the experimental measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalysis_data[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the measurements as vector by flattening the matrix:\n",
    "$$\n",
    "y = \\operatorname{vec}(Y)\\in\\mathbb{R}^{30}.\n",
    "$$\n",
    "Note that ``vec`` is the vectorization operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the connection between the solution of the dynamical system $z(t,x)$ and the experimental data?\n",
    "It is as follows:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "z_1(30j;x) &\\longrightarrow& Y_{j1},\\\\\n",
    "z_2(30j;x) &\\longrightarrow& Y_{j2},\\\\\n",
    "z_4(30j;x) &\\longrightarrow& Y_{j3},\\\\\n",
    "z_5(30j;x) &\\longrightarrow& Y_{j4},\\\\\n",
    "z_6(30j;x) &\\longrightarrow& Y_{j5},\n",
    "\\end{array}\n",
    "$$\n",
    "for $j=1,\\dots,6$.\n",
    "\n",
    "We are now ready to define a function:\n",
    "$$\n",
    "f:\\mathcal{X} \\rightarrow \\mathcal{Y}=\\mathbb{R}^{30}_+,\n",
    "$$\n",
    "as follows:\n",
    "+ Define the matrix function:\n",
    "$$\n",
    "F:\\mathcal{X} \\rightarrow \\mathbb{R}^{5\\times 6},\n",
    "$$\n",
    "by:\n",
    "$$\n",
    "\\begin{array}{ccccc}\n",
    "F_{j1}(x) &=& z_1(30j;x)&\\longrightarrow& Y_{j1},\\\\\n",
    "F_{j2}(x) &=& z_2(30j;x) &\\longrightarrow& Y_{j2},\\\\\n",
    "F_{j3}(x) &=& z_4(30j;x) &\\longrightarrow& Y_{j3},\\\\\n",
    "F_{j4}(x) &=& z_5(30j;x) &\\longrightarrow& Y_{j4},\\\\\n",
    "F_{j5}(x) &=& z_6(30j;x) &\\longrightarrow& Y_{j5},\n",
    "\\end{array}\n",
    "$$\n",
    "+ And flatten that function:\n",
    "$$\n",
    "f(x) = \\operatorname{vec}(F(x))\\in\\mathbb{R}^{30}.\n",
    "$$\n",
    "\n",
    "Now, we have made the connection with our theoretical formulation of inverse problems crystal clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Programming our ODE solver and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate\n",
    "\n",
    "def A(x):\n",
    "    \"\"\"\n",
    "    Return the matrix of the dynamical system.\n",
    "    \"\"\"\n",
    "    # Scale back to the k's\n",
    "    k = np.exp(x) / 180.\n",
    "    res = np.zeros((6,6))\n",
    "    res[0, 0] = -k[0]\n",
    "    res[1, 0] = k[0]\n",
    "    res[1, 1] = -(k[1] + k[3] + k[4])\n",
    "    res[2, 1] = k[1]\n",
    "    res[2, 2] = -k[2]\n",
    "    res[3, 2] = k[2]\n",
    "    res[4, 1] = k[4] # swapping\n",
    "    res[5, 1] = k[3]\n",
    "    return res\n",
    "    \n",
    "\n",
    "def g(z, t, x):\n",
    "    \"\"\"\n",
    "    The right hand side of the dynamical system.\n",
    "    \"\"\"\n",
    "    return np.dot(A(x), z)\n",
    "\n",
    "\n",
    "# The initial conditions\n",
    "z0 = np.array([500., 0., 0., 0., 0., 0.])\n",
    "\n",
    "\n",
    "# The times at which we need the solution (experimental times)\n",
    "t_exp = np.array([30. * j for j in range(1, 7)])\n",
    "\n",
    "# The experimental data as a matrix\n",
    "Y = catalysis_data[1:].values[:, 1:]\n",
    "\n",
    "# The experimental as a vector\n",
    "y = Y.flatten()\n",
    "\n",
    "# The full solution of the dynamical system\n",
    "def Z(x, t):\n",
    "    \"\"\"\n",
    "    Returns the solution for parameters x at times t.\n",
    "    \"\"\"\n",
    "    return scipy.integrate.odeint(g, z0, t, args=(x,))\n",
    "\n",
    "\n",
    "# The matrix function F (matches to Y)\n",
    "def F(x, t):\n",
    "    res = Z(x, t)\n",
    "    return np.hstack([res[:, :2], res[:, 3:]])\n",
    "    \n",
    "\n",
    "# The function f (matches to y)\n",
    "def f(x, t):\n",
    "    return F(x, t).flatten()\n",
    "\n",
    "# Finally, the loss function that we need to minimize over x:\n",
    "def L(x, t, y):\n",
    "    return 0.5 * np.sum((f(x, t) / 500. - y / 500.) ** 2) # We scale for numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Minimize the loss function\n",
    "\n",
    "Let's optimize with scipy.optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "# Initial guess for x\n",
    "x0 = -2.0 + 2.0 * np.random.rand(5)\n",
    "\n",
    "# Optimize\n",
    "res = scipy.optimize.minimize(L, x0, args=(t_exp, y))\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how you can visualize the model with the \"best\" parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = res.x\n",
    "t = np.linspace(0, 180, 100)\n",
    "x1 = np.array([1.359, 1.657, 1.347, -.16, -1.01])\n",
    "Yp = Z(x, t)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "catalysis_data.plot(ax=ax, style='s', x=0)\n",
    "ax.plot(t, Yp[:, 0], color=sns.color_palette()[0], label='Model NO3-')\n",
    "ax.plot(t, Yp[:, 1], color=sns.color_palette()[1], label='Model NO2-')\n",
    "ax.plot(t, Yp[:, 2], color=sns.color_palette()[5], label='Model X')\n",
    "ax.plot(t, Yp[:, 3], color=sns.color_palette()[2], label='Model N2')\n",
    "ax.plot(t, Yp[:, 4], color=sns.color_palette()[3], label='Model NH3')\n",
    "ax.plot(t, Yp[:, 5], color=sns.color_palette()[4], label='Model N2O')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code above will not work every time... Some times it will work and sometimes it won't work.\n",
    "Run it 3-4 times if it accidentally works.\n",
    "There are several problems.\n",
    "Here are the three most relevant in our context:\n",
    "+ scipy.optimize needs the gradient of the loss function. Since we do not provide it, it tries to get it using numerical differentiation. Numerical differentiation introduces errors...\n",
    "+ scipy.optimize find a local minimum of the loss function. This may actually be a bad local minimum.\n",
    "+ okay, this particular model is not very computationally expensive. But imagine trying to calibrate a model that takes a while for a single evaluation (e.g., a finite element model). Then, using scipy.optimize (especially without supplying the derivatives), is doomed to fail.\n",
    "\n",
    "To overcome these difficulties, you have to use Bayesian global optimization to solve the problem.\n",
    "Note that in the hands-on activities, we introduced this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ei(m, sigma, ymax, psi=0.):\n",
    "    u = (m - ymax) / sigma\n",
    "    ei = sigma * (u * st.norm.cdf(u) + st.norm.pdf(u))\n",
    "    ei[sigma <= 0.] = 0.\n",
    "    return ei\n",
    "\n",
    "def maximize(f, gpr, domain, num_candidates=10000,\n",
    "             alpha=ei, psi=0., max_it=6):\n",
    "    \"\"\"\n",
    "    Optimize f using a limited number of evaluations.\n",
    "    \n",
    "    :param f:        The function to optimize.\n",
    "    :param gpr:      A Gaussian process model to use for representing our state of knowldege.\n",
    "    :param X_design: The set of candidate points for identifying the maximum.\n",
    "    :param alpha:    The acquisition function.\n",
    "    :param psi:      The parameter value for the acquisition function (not used for EI).\n",
    "    :param max_it:   The maximum number of iterations.\n",
    "    \"\"\"\n",
    "    af_all = []\n",
    "    print('Iteration\\tCurrent best objective \\tCurrent acquisition func. value')\n",
    "    dim = gpr.X.shape[1]\n",
    "    for count in range(max_it):\n",
    "        X_design = domain[:, 0] + \\\n",
    "                   (domain[:, 1] - domain[:, 0]) * \\\n",
    "                        np.random.rand(num_candidates, dim)\n",
    "        m, sigma2 = gpr.predict(X_design)\n",
    "        sigma = np.sqrt(sigma2)\n",
    "        af_values = alpha(m, sigma, gpr.Y.max(), psi=psi)\n",
    "        i = np.argmax(af_values)\n",
    "        X = np.vstack([gpr.X, X_design[i:(i+1), :]])\n",
    "        y = np.vstack([gpr.Y, [f(X_design[i, :])]])\n",
    "        gpr.set_XY(X, y)\n",
    "        # Uncomment the following to optimize the hyper-parameters\n",
    "        #gpr.optimize()\n",
    "        idx_opt = np.argmax(gpr.Y.flatten())\n",
    "        f_opt = gpr.Y[idx_opt, 0]\n",
    "        print('{0:d}\\t\\t{1:1.2f}\\t\\t\\t{2:1.2f}'.format(count + 1, f_opt, af_values[i, 0]))\n",
    "    x_opt = np.array(gpr.X[idx_opt])\n",
    "    return x_opt, f_opt, gpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code *maximizes* a function, but you want to *minimize* the loss.\n",
    "To recast the problem as a maximization problem, you need to work with *minus the loss*.\n",
    "Also, the code does not allow for a function with extra parameters (like the ``t_exp`` and the ``y`` we have for ``L``).\n",
    "Here is the function that you should be optimizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = lambda x: -L(x, t_exp, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Perform multivariate Gaussian process regression on an initial set of data\n",
    "\n",
    "We are going to search for the best parameters $x$ within the set $[-2,2]^5$.\n",
    "Consider the following two datasets consisting of parameter and minus loss pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial training points\n",
    "n_init_train = 100\n",
    "X_init_train = -2.0 + 4.0 * np.random.rand(n_init_train, 5)\n",
    "Y_init_train = np.array([h(x) for x in X_init_train])[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a squared exponential covariance function with automatic relevance determination to do Gaussian process regression with ``X_init_train`` and ``Y_init_train``.\n",
    "\n",
    "Hint: You may want to experiment by constraining the likelihood noise of your model to be very small, say $10^{-6}$. This is because the observations of the loss do not really have any noise.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Inspecting your model\n",
    "\n",
    "Use the lengthscale information to rank the model parameters according their effect on the calibration loss.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Diagnostics\n",
    "\n",
    "Here are some test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test points\n",
    "n_test = 50\n",
    "X_test = -2.0 + 4.0 * np.random.rand(n_test, 5)\n",
    "Y_test = np.array([h(x) for x in X_test])[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "\n",
    "+ Predictions vs observations plot\n",
    "+ Standarized errors plot\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - Calibrate the model with Bayesian global optimization\n",
    "\n",
    "Now use Bayesian global optimization with expected improvement to calibrate your model using the GP that you built above as the starting point.\n",
    "Do not expect this to give you a perfect model.\n",
    "But it will be better than nothing.\n",
    "We will get the best possible model in the next homework assignment.\n",
    "\n",
    "Hint: Here you basically need to read the docstring of ``maximize`` and use it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your convenience, the `domain` argument of minimize should be:\n",
    "domain = np.array([[-2, 2], [-2, 2], [-2, 2], [-2, 2], [-2, 2]])\n",
    "# Run maximize here:\n",
    "# Your code here\n",
    "x_opt = ? # The best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this code to plot your calibrated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_opt\n",
    "t = np.linspace(0, 180, 100)\n",
    "x1 = np.array([1.359, 1.657, 1.347, -.16, -1.01])\n",
    "Yp = Z(x, t)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "catalysis_data.plot(ax=ax, style='s', x=0)\n",
    "ax.plot(t, Yp[:, 0], color=sns.color_palette()[0], label='Model NO3-')\n",
    "ax.plot(t, Yp[:, 1], color=sns.color_palette()[1], label='Model NO2-')\n",
    "ax.plot(t, Yp[:, 2], color=sns.color_palette()[5], label='Model X')\n",
    "ax.plot(t, Yp[:, 3], color=sns.color_palette()[2], label='Model N2')\n",
    "ax.plot(t, Yp[:, 4], color=sns.color_palette()[3], label='Model NH3')\n",
    "ax.plot(t, Yp[:, 5], color=sns.color_palette()[4], label='Model N2O')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
